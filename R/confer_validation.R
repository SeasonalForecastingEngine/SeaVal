

#' Assess score uncertainty by bootstrapping
#'
#' @param dt Data table containing scores.
#' @param r how many bootstrap-resamples of the mean score should be considered?
#' @param score_col column name of the scores
#' @param by_cols should the mean score be aggregated? E.g. if you have a data table with scores from different systems,
#' putting 'system' here gives you a bootstrap estimate for the mean score of each system, which is usually what you want
#' @export

bootstrap_scores_dt = function(dt,r = 100,score_col,bycols = NULL,mc_cores = 1)
{

  statistic = function(x,inds){mean(x[inds],na.rm = T)}

  bootstrap_dt = data.table(R = 1:r)

  if(length(bycols) > 0)
  {
    for(i in 1:length(bycols))
    {
      bootstrap_dt = c(bootstrap_dt,unique(dt[,.SD,.SDcols = bycols[i]]))
    }
  }

  bootstrap_dt = as.data.table(expand.grid(bootstrap_dt))

  if(length(bycols) > 0)
  {
    by_dt = unique(dt[,.SD,.SDcols = bycols])
    setkeyv(by_dt,names(by_dt))
    setkeyv(dt,key(by_dt))
    setkeyv(bootstrap_dt,key(by_dt))

    aux_fun = function(row_index)
    {
      sub_dt = by_dt[row_index,]
      data = dt[sub_dt,get(score_col)]
      samples = boot::boot(data,statistic = statistic,R = r)$t

      bootstrap_dt[sub_dt,bootstrap_samples := samples]
    }

    if(mc_cores == 1)
    {
      for(ri in 1:by_dt[,.N])
      {
        aux_fun(ri)
      }
    } else { # using more cores doesn't speed up things here, so some bad coding happened?
      parallel::mclapply(1:by_dt[,.N],aux_fun,mc.cores = mc_cores)
    }
  } else {
    bootstrap_dt[,bootstrap_samples := boot::boot(dt[,get(score_col)],statistic = statistic,R = r)$t]
  }

  return(bootstrap_dt)
}

#' Calculate exceedence Brier score
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_col column name of the prediction. Contains predicted probabilities of exceedence
#' @param threshold_col which column contains the exceedence threshold?
#' @param obs_col column name of the observations. Can either be logical (exceedence or not) or real valued, containing precipitation (or the variable for which exceedence should be checked).
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @export

BS_ex_dt = function(fc_dt,fc_col,threshold_col,
                    obs_col = 'obs',
                    by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                    along_cols = c('year'))
{
  if(range(fc_dt[,get(fc_col)],na.rm = T)[2] > 1) stop('your predictions should be values between 0 and 1.')
  if(is.logical(fc_dt[,get(obs_col)]))
  {
    Score_dt = fc_dt[,.(BS_ex = (get(fc_col)-get(obs_col))^2),by = c(by_cols,threshold_col)]# the by-arguments are for keeping these columns only
  }
  if(!is.logical(fc_dt[,get(obs_col)]))
  {
    Score_dt = fc_dt[,.(BS_ex = (get(fc_col) - (get(obs_col) > get(threshold_col)))^2),by = c(by_cols,threshold_col)]
  }
  return(Score_dt)
}



#' Calculate exceedence Brier skill score
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_col column name of the prediction. Contains predicted probabilities of exceedence
#' @param clim_col column name containing the out-of-sample climatology forecast for the exceedence probability.
#' Usually generated by \code{climatology_threshold_exceedence}.
#' @param threshold_col which column contains the exceedence threshold?
#' @param obs_col column name of the observations. Can either be logical (exceedence or not) or real valued, containing precipitation (or the variable for which exceedence should be checked).
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @export

BSS_ex_dt = function(fc_dt,
                     fc_col,
                     clim_col = 'clim',
                     threshold_col,
                     obs_col = 'obs',
                     by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)))
{

  BS_dt = BS_ex_dt(fc_dt = fc_dt,
                   fc_col = fc_col,
                   threshold_col = threshold_col,
                   obs_col = obs_col,
                   by_cols = by_cols)

  clim_es_dt = BS_ex_dt(fc_dt = fc_dt,
                        fc_col = clim_col,
                        threshold_col = threshold_col,
                        obs_col = obs_col,
                        by_cols = by_cols)


  setnames(clim_es_dt,'BS_ex','clim_BS_ex')
  score_dt = merge(BS_dt,clim_es_dt,by = intersect(names(BS_dt),names(clim_es_dt)))

  score_dt[,BSS_ex := (clim_BS_ex - BS_ex)/clim_BS_ex ]

  #deal with divisions by 0:
  score_dt[clim_BS_ex == 0 & BS_ex == 0, BSS_ex := 0] # when both are perfect, skill score should be 0
  score_dt[clim_BS_ex == 0 & BS_ex > 0, BSS_ex := -1] # set to something negative (capped at -1), when climatology predicts perfectly and the forecast does not
  #(should actually -Inf, but that's bad for plotting).
  return(score_dt)
}




#' Returns a leave-one-year-out climatology-based ensemble forecast
#'
#' for a given year, the ensemble forecast simply consists of the observations in all other years.
#' This is essentially an auxiliary function for computing skill scores relative to climatology.
#'
#' @param obs_dt Data table containing observations, must contain a column 'year'.
#' @param by_cols character vector containing the column names of the grouping variables, e.g. \code{c('month','lon','lat')}.
#'
#' @return Long data table with the typical ensemble-forecast looks, i.e. containing a column 'member'.
#' @export

climatology_ens_forecast = function(obs_dt,
                                    by_cols)
{
  years = unique(obs_dt[,year])

  ret_dt = data.table()
  for(yy in years)
  {
    dt_temp = obs_dt[year != yy][,member := 1:.N,by = by_cols][,year:=yy]
    ret_dt = rbindlist(list(ret_dt,dt_temp))
  }
  return(ret_dt)
}




#' Get climatological prediction for exceedence probabilities.
#'
#' The climatological prediction for exceedence probabilities is the fraction of observed years where the observation exceeded the threshold.
#' It's calculated from leave-one-year-out climatology.
#'
#' @param obs_dt Data table containing observations.
#' @param obs_col column name of the observation. Mostly observed precipitation in mm.
#' @param by_cols By which columns should be grouped?
#' @param thresholds vector of thresholds for which the exceedence probabilities should be derived.
#'
#' @export

climatology_threshold_exceedence = function(obs_dt,
                                            obs_col = 'prec',
                                            by_cols = intersect(c('lon','lat','month','season'),names(obs_dt)),
                                            thresholds = c(200,300,350,400))
{
  clim_dt = climatology_ens_forecast(obs_dt,by_cols = by_cols)
  ret_dt = data.table()
  for(thr in thresholds)
  {
    thr_dt = clim_dt[,.(pexcd = mean(get(obs_col) > thr)),by = c(by_cols,'year')]
    thr_dt[,threshold := thr]
    ret_dt = rbindlist(list(ret_dt,thr_dt))
  }
  return(ret_dt)

}



#' composite analysis for teleconnections
#'
#'@param var_dt Data table containing the weather variables for which you want to conduct the composite analysis, e.g. monthly mean precip for a range of years, months, and locations.
#'@param TC_dt Data table containing the values of the teleconnection index (such as IOD), typically indexed by year and month.
#'@param TC_name Character string containing the column name of TC_dt where the values of the teleconnection index are stored. Default is third column.
#'@param by_cols Character vector containing the column names of the grouping variables by which the composite analysis is supposed to be conducted. Default is \code{c('month','lon','lat')}.
#'@param var_name Column name of the weather variable in var_dt. Default is first column not contained in \code{by_cols} and not named 'year'.
#'
#'@return A data table containing the composites x_plus and x_minus for each value of by_cols
#'@author Claudio
#'@export


composite_analysis = function(var_dt,TC_dt,
                              TC_name = names(TC_dt)[3],
                              by_cols = c('month','lon','lat'),
                              average_along_cols = 'year',
                              var_name = setdiff(names(var_dt),c(average_along_cols,by_cols))[1])
{
  # get three categories: -1 is low TC, 0 is normal TC, 1 is high TC
  TC_by = intersect(by_cols,names(TC_dt))
  TC_dt[,cat := -1*(get(TC_name) <= quantile(get(TC_name),0.33)) + 1*(get(TC_name) >= quantile(get(TC_name),0.67)),by = TC_by]

  TCcols = intersect(c('cat',by_cols,average_along_cols),names(TC_dt))
  var_dt = merge(var_dt,TC_dt[,.SD,.SDcols = TCcols],by = c(average_along_cols,TC_by))

  CA_dt = var_dt[,mean(get(var_name)),by = c(by_cols,'cat')]
  setnames(CA_dt,'V1',var_name)
  setkeyv(CA_dt,c(by_cols,'cat'))

  ret_dt = unique(CA_dt[,.SD,.SDcols = by_cols])
  x_plus = CA_dt[ cat == 1,get(var_name)] - CA_dt[ cat == 0,get(var_name)]
  x_minus = CA_dt[ cat == -1,get(var_name)] - CA_dt[ cat == 0,get(var_name)]

  ret_dt[,x_plus := x_plus][,x_minus := x_minus]

  return(ret_dt)
}


#' Taking CRPSs of ensemble forecasts stored in long data tables:
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_cn column name of the prediction.
#' @param obs_dt Data table containing the observations. Pass NULL if the observations are contained in fc_dt.
#' @param obs_cn column name of the observations (either in obs_dt, or in fc_dt if obs_dt = NULL).
#' @param by_cns column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @param along_cns column name(s) for the variable(s) along which is averaged, typically just 'year'.
#' @param check_dimension Logical. If True, a simple test whether the dimensions match up is conducted:
#' Namely, getting the mean grouped by by_cns AND along_cns should correspond to getting the ensemble mean. This is tested.
#' @param member_cn Name of the column identifying the ensemble member.
#'
#' @export


CRPS_ens_fc = function(fc_dt,fc_cn,
                       obs_cn = 'obs',
                       by_cns = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                       along_cns = c('year'),
                       check_dimensions = T,
                       member_cn = 'member')
{
  fc_dt = fc_dt[!is.na(get(obs_cn)) & ! is.na(get(fc_cn))]

  if(check_dimensions)
  {
    check = unique(fc_dt[,.SD,.SDcols = c(by_cns,along_cns,member_cn)])[,.N] == fc_dt[,.N]
    if(!check) stop('dimension check failed: your data format does not meet the requirements.')
  }


  #expand for application of crps:
  if(!is.null(by_cns))
  {
    ff = paste0('year + ',obs_cn,' + ',paste(by_cns,collapse = ' + '),' ~ ',member_cn)
  } else {
    ff = paste0('year + ',obs_cn,'~ ',member_cn)
  }

  fc_dt_new = dcast(fc_dt,formula = as.formula(ff),fun.aggregate = mean,value.var = fc_cn,na.rm = T)

  pred_cols = (length(by_cns) + length(along_cns) + 2) : ncol(fc_dt_new) # that's a bit hacked

  pred_mat = as.matrix(fc_dt_new[,pred_cols,with = FALSE])

  obs = fc_dt_new[,get(obs_cn)]

  crps_vals = crps_sample_na(obs,pred_mat)

  ret_dt = fc_dt_new[,.SD,.SDcols = c(along_cns,by_cns)][,CRPS := crps_vals]
  ret_dt = ret_dt[,.(CRPS = mean(CRPS)), by = by_cns]
  return(ret_dt)
}

#' Function for taking CRPS skill scores of ensemble forecasts stored in long data tables:
#'
#' Warning: The skill score needs a climatological forecast as reference. This is so far always based on the leave-one-year-out climatology.
#' @param fc_dt Data table containing the predictions.
#' @param fc_cn column name of the prediction.
#' @param obs_dt Data table containing the observations. Pass NULL if the observation are contained in fc_dt.
#' @param obs_cn column name of the observations (either in obs_dt, or in fc_dt if obs_dt = NULL).
#' @param by_cns column names of grouping variables, all of which need to be columns in fc_dt. A separate CRPS is computed for each value of the grouping variables.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @param along_cns column name(s) for the variable(s) along which is averaged. Needs to contain 'year' since the reference climatology forecast is leave-one-year-out.
#' @param ... passed on to CRPS_ens_fc
#'
#' @export

CRPSS_ens_fc = function(fc_dt,fc_cn,
                        obs_cn = 'obs',
                        by_cns = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                        along_cns = c('year'),...)
{

  if(!('year' %in% along_cns)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')

  obs_dt = unique(fc_dt[,.SD,.SDcols = intersect(c(obs_cn,by_cns,along_cns),c('year','month','lon','lat',obs_cn))])
  # note that by_cns can contain e.g. different systems, all of which are compared to the same observation, therefore the intersect.

  obs_by_cns = intersect(by_cns,names(obs_dt))

  climatology_prediction = climatology_ens_forecast(obs_dt = obs_dt,
                                                    by_cols = obs_by_cns)
  setnames(climatology_prediction,'obs','clim')

  climatology_prediction = merge(climatology_prediction,obs_dt,by = c(obs_by_cns,'year'))

  climatology_CRPS = CRPS_ens_fc(fc_dt = climatology_prediction,
                               fc_cn = 'clim',
                               obs_cn = obs_cn,
                               by_cns = obs_by_cns)

  setnames(climatology_CRPS,'CRPS','clim_CRPS')

  CRPS_dt = CRPS_ens_fc(fc_dt = fc_dt,
                      fc_cn = fc_cn,
                      obs_cn = obs_cn,
                      by_cns = by_cns,
                      along_cns = along_cns,...)

  CRPS_dt = merge(CRPS_dt,climatology_CRPS)
  CRPS_dt[,CRPSS := (clim_CRPS - CRPS)/clim_CRPS]

  return(CRPS_dt)

}


#' Auxiliary function for computing CRPS on ensemble forecasts
#' Essentially wraps scoringRules::crps_sample, but can deal with missing values.
#' That's important when your ensemble switches size between hind- and forecasts, and
#' when you're computing crps for several systems at once and are too lazy to peel them apart.
#'
#' @param obs vector of observations.
#' @param pred either vector of predictions when a single observation is provided, or a matrix with nrow = length(obs)
#' where the different columns are the different predictions.
#'
#' @importFrom scoringRules crps_sample
#' @export

crps_sample_na = function(obs, pred)
{
  crps_sample_na_single_obs = function(single_obs,vector_pred)
  {
    vector_pred2 = vector_pred[!(is.na(vector_pred) | is.nan(vector_pred))]
    crps = scoringRules::crps_sample(single_obs,vector_pred2)
    return(crps)
  }

  if(length(obs) == 1)
  {
    crps = crps_sample_na_single_obs(obs,pred)
  } else{
    crps = sapply(seq_along(obs), function(i) crps_sample_na_single_obs(obs[i], pred[i,]))
  }

  return(crps)
}

#' Compute the Multicategory Brier Skill score
#'
#' This score is suitable for tercile category forecasts.
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_cols column names of the prediction.
#' @param obs_col column name of the observations (either in obs_dt, or in fc_dt if obs_dt = NULL). The observation column needs to
#' contain -1 if it falls into the first category (corresponding to fc_cols[1]), 0 for the second and 1 for the third category.
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @param along_cns column name(s) for the variable(s) along which is averaged, typically just 'year'.
#' @export


MBSS_dt = function(fc_dt,fc_cols = c('below','normal','above'),
                  obs_col = ifelse(is.null(obs_dt),yes = 'obs',no = fc_col),
                  by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                  along_cols = c('year'))
{
# Multicategory Brier skill score:

  MBSS_dt = fc_dt[,.(MBSS = 3/2 * (2/3 - mean((get(fc_cols[1]) - (get(obs_col) == -1))^2 + (get(fc_cols[2]) - (get(obs_col) == 0))^2 + (get(fc_cols[3]) - (get(obs_col) == 1))^2))),by = by_cols]
  return(MBSS_dt)
}




#' Taking MSEs of ensemble forecasts stored in long data tables. Can also handle point forecast
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_cn column name of the prediction.
#' @param obs_dt Data table containing the observations. Pass NULL if the observation are contained in fc_dt.
#' @param obs_cn column name of the observations (either in obs_dt, or in fc_dt if obs_dt = NULL).
#' @param by_cns column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @param along_cns column name(s) for the variable(s) along which is averaged, typically just 'year'.
#' @param check_dimension Logical. If True, a crude test whether the data table is in the correct shape is conducted
#' Namely, getting the mean grouped by by_cns AND along_cns should correspond to getting the ensemble mean. This is tested.
#' @param member_cn Name of the column identifying the ensemble member. Only used if check_dimension is TRUE. Is NULL for a point forecast.
#' @export


MSE_dt = function(fc_dt,fc_col,
                  obs_col = ifelse(is.null(obs_dt),yes = 'obs',no = fc_col),
                  by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                  along_cols = c('year'),
                  check_dimensions = T,
                  member_col = intersect('member',colnames(fc_dt)))
{
  if(check_dimensions)
  {
    check = unique(fc_dt[,.SD,.SDcols = c(by_cols,along_cols,member_col)])[,.N] == fc_dt[,.N]
    if(!check) stop('Dimension check failed. Probably you have multiple layers per ensemble member and year after grouping.')
  }

  fc_dt_new = copy(fc_dt)

  # get forecast mean (= mean over all ensemble members)
  fc_dt_new[,fc_mean := mean(get(fc_col),na.rm = T),by = c(by_cols,along_cols)]


  # take mean along all
  MSE_dt = fc_dt_new[,.(MSE = mean((fc_mean - get(obs_col))^2,na.rm = T)),by = by_cols]
  return(MSE_dt)
}



#' Function for taking MSE skill scores of ensemble forecasts stored in long data tables:
#'
#' Can also handle point forecasts.
#' Warning: The skill score needs a climatological forecast as reference. This is so far always based on the leave-one-year-out climatology.
#' @param fc_dt Data table containing the predictions.
#' @param fc_col column name of the prediction.
#' @param obs_dt Data table containing the observations. Pass NULL if the observation are contained in fc_dt.
#' @param obs_col column name of the observations (either in obs_dt, or in fc_dt if obs_dt = NULL).
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt. A separate MSE is computed for each value of the grouping variables.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @param along_cols column name(s) for the variable(s) along which is averaged. Needs to contain 'year' since the reference climatology forecast is leave-one-year-out.
#' @param ... passed on to MSE_dt
#'
#' @export

MSESS_dt = function(fc_dt,fc_col,
                    obs_col = 'obs',
                    by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
                    along_cols = c('year'),...)
{

  if(!('year' %in% along_cols)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')

  obs_dt = unique(fc_dt[,.SD,.SDcols = intersect(c(obs_col,by_cols,along_cols),c('year','month','season','lon','lat',obs_col))])
  # note that by_cols can contain e.g. different systems, all of which are compared to the same observation, therefore the intersect.


  obs_by_cols = intersect(by_cols,names(obs_dt))

  climatology_prediction = climatology_ens_forecast(obs_dt = obs_dt,
                                                    by_cols = obs_by_cols)
  setnames(climatology_prediction,obs_col,'clim')

  climatology_prediction = merge(climatology_prediction,obs_dt,by = c(obs_by_cols,'year'))

  climatology_MSE = MSE_dt(fc_dt = climatology_prediction,
                           fc_col = 'clim',
                           obs_col = obs_col,
                           by_cols = obs_by_cols)

  setnames(climatology_MSE,'MSE','clim_MSE')

  MSE_dt = MSE_dt(fc_dt = fc_dt,
                  fc_col = fc_col,
                  obs_col = obs_col,
                  by_cols = by_cols,
                  along_cols = along_cols,...)

  MSE_dt = merge(MSE_dt,climatology_MSE, by = intersect(names(MSE_dt),names(climatology_MSE)))
  MSE_dt[,MSESS := (clim_MSE - MSE)/clim_MSE]

  return(MSE_dt)
}



#' Function for calculating Pearson correlation coefficients (PCCs) of ensemble mean forecasts stored in long data tables:
#'
#' Can also handle point forecasts.
#' Warning: This metric always needs several years of data since the means and standard deviations are calculated across time.
#' @param fc_obs_dt Data table containing the predictions.
#' @param fc_col column name of the prediction.
#' @param obs_col column name of the observations.
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt. A separate PCC is computed for each value of the grouping variables.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_obs_dt.
#' @param along_cols column name(s) for the variable(s) along which is averaged. Needs to contain 'year' per warning above.
#' @param ... passed on to PCC_dt
#'
#' @export

PCC_dt = function(fc_obs_dt, fc_col,
                  obs_col = 'obs',
                  by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_obs_dt)),
                  along_cols = c('year'))
{

  if(!('year' %in% along_cols)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')

  # get forecast mean (= mean over all ensemble members)
  fc_obs_dt[, fc_mean:=mean(get(fc_col),na.rm = T), by=c(by_cols,along_cols)]

  # calculate correlation coefficient
  fc_obs_dt = fc_obs_dt[, .SD, .SDcols=c("fc_mean",obs_col,by_cols,along_cols)]
  PCC_dt = fc_obs_dt[, .(rho=cor(fc_mean,get(obs_col),use="na.or.complete")), by=by_cols]

  return(PCC_dt)
}



#' Function for calculating coefficients of predictive ability (CPAs) of ensemble mean forecasts stored in long data tables:
#'
#' Can also handle point forecasts.
#' Warning: This metric always needs several years of data since the ranks on which it is based are calculated across multi-year samples.
#' @param fc_obs_dt Data table containing the predictions.
#' @param fc_col column name of the prediction.
#' @param obs_col column name of the observations.
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt. A separate CPA is computed for each value of the grouping variables.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_obs_dt.
#' @param along_cols column name(s) for the variable(s) along which is averaged. Needs to contain 'year' per warning above.
#' @param ... passed on to CPA_dt
#'
#' @export

CPA_dt = function(fc_obs_dt, fc_col,
                  obs_col = 'obs',
                  by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_obs_dt)),
                  along_cols = c('year'),...)
{

  if(!('year' %in% along_cols)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')


  # get forecast mean (= mean over all ensemble members)
  fc_obs_dt[, fc_mean:=mean(get(fc_col), na.rm=T), by=c(by_cols,along_cols)]

  # combine data into a single DT and remove rows with missing data
  fc_obs_dt = na.omit(fc_obs_dt[, .SD, .SDcols=c("fc_mean",obs_col,by_cols,along_cols)])

  # calculate the CPA
  fc_obs_dt[, fc_midrank:=frank(fc_mean,ties.method="average"), by=by_cols]
  fc_obs_dt[, obs_class:=frank(get(obs_col),ties.method="dense"), by=by_cols]
  fc_obs_dt[, obs_midrank:=frank(get(obs_col),ties.method="average"), by=by_cols]
  CPA_dt = fc_obs_dt[, .(cpa=0.5*(1.+cov(obs_class,fc_midrank)/cov(obs_class,obs_midrank))), by=by_cols]

  return(CPA_dt)
}




