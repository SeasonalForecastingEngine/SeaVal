# Data import and processing

```{r setup, echo = F, message = F}
knitr::opts_chunk$set(fig.width = 5)
knitr::opts_chunk$set(fig.height = 6)
library(SeaVal)
library(ggpubr)
library(ggplot2)
load(file = '_data_dir.RData')

```



## Reshaping data {#data-examples}

For forecast validation, the ideal data format is to have all your fore- and hindcasts in the same data table, alongside the corresponding observations. So one column of forecasts, one column of observations and several columns of dimension variables (e.g. year, month, lon,lat). However, this is rarely how your netcdf-data looks like: you'll often have different netcdfs for observations and fore-/hindcasts. They might, moreover have different units, different missing values, different variable names etc.

So to get your data into the preferred shape, you either need to manipulate the netcdf files beforehand, to get exactly the data table you want from `netcdf_to_dt`, or you can extract several data tables and do the required manipulations in `R`, using `SeaVal` and `data.table`. In this section we show a few examples for this.

### Example: cross-validation data {#cv-data}

In our example folder (see above) we have two crossvalidation datasets and corresponding observations (one for FMA, one for MAM). 
We will process them simultaneously here, merging everything into one single data table.  
This is not really making things easier and not generally recommended. It is a great way for us to highlight more `data.table`-syntax, though.

```{r}
# get the two CV-files:
fn_pred1 = "CrossValidatedPredictedRain_Feb-Apr_Feb2021.nc"
fn_pred2 = "CrossValidatedPredictedRain_Mar-May_Feb2021.nc"

dt_pred1 = netcdf_to_dt(paste0(data_dir,fn_pred1),verbose = 0) # they look the same, we can just look at the information from one of them:
dt_pred2 = netcdf_to_dt(paste0(data_dir,fn_pred2))

#this is how our data looks now:
print(dt_pred1)

# before joining the two data tables, we should add a column identifying which is which:
dt_pred1[,season:= 'FMA']
dt_pred2[,season:= 'MAM']

# bind together
dt_pred = rbindlist(list(dt_pred1,dt_pred2))
print(dt_pred)
```

The function `rbindlist` binds a list of several data tables into one. This only works if they have the same columns, though, otherwise you need to use `merge`.

```{r}
# next, get the observations:
fn_obs1 = "ObservedRain_Feb-Apr_Feb2021.nc"
fn_obs2 = "ObservedRain_Mar-May_Feb2021_update.nc"
dt_obs1 = netcdf_to_dt(paste0(data_dir,fn_obs1),verbose = 0)
dt_obs2 = netcdf_to_dt(paste0(data_dir,fn_obs2) )

dt_obs1[,season := 'FMA']
dt_obs2[,season := 'MAM']
dt_obs = rbindlist(list(dt_obs1,dt_obs2))
```

Now we have two data tables, one with predictions and one with observations. We want to join them, but we want to have predictions next to observations, so `rbindlist` does not work for us here, and we need to use `merge`. However, we should first make sure that the columns are named appropriately: Currently, both `dt_obs` and `dt_pred` have a column named `prec`.

```{r}
setnames(dt_pred,'prec','prediction')
setnames(dt_obs,'prec','observation')

dt = merge(dt_pred,dt_obs,by = c('lon','lat','time','season'))
print(dt)

# remove all rows with missing predictions:
dt = dt[!is.na(prediction)]

# convert time from the 'months since date' (MSD) format to years and months (YM)
dt = MSD_to_YM(dt,origin = '1981-01-01') # (the origin was documented in the netcdf, see above.)
print(dt) 
```
We now have the data table in the shape we want it to be, containing both predictions and observations as one column each. In section \@ref(cv-eval) we show how to evaluate the predictions.

```{r,echo = FALSE}
# for use in the next section:
dt_cv = copy(dt)
```

### Example: 'corrupted' netcdf{#ex-corrupted-netcdf}

Data handling can be messy and things can go wrong at any stage. Here, we have a look at a netcdf file where something has gone wrong:

```{r,error = TRUE}
fn = "PredictedProbabilityRain_Feb-Apr_Feb2021.nc"
dt = netcdf_to_dt(paste0(data_dir,fn))
```

The `netcdf_to_dt` function prints out the netcdf information, and then crashes with the error message above, saying that we have disjoint dimension variables for some variables. Indeed, looking at the printed out netcdf-description, we have three variables (below,normal,above), and while 'below' and 'above' are indexed by 'lon' and 'lat', 'normal' is indexed by 'ncl3' and 'ncl4'. As the error message suggests, we can set `trymerge` to FALSE, making `netcdf_to_dt` return a list of data tables. 

```{r}
dt_list = netcdf_to_dt(paste0(data_dir,fn),trymerge = FALSE,verbose = 0)
print(dt_list)
```

We see that 'ncl3' and 'ncl4' have different values than 'lon' and 'lat', apparently they are meaningless indexing integers. However, the three data.tables are of the same size, and we can hope that the 'below' data table is arranged in the same row-ordering than the others. If this is the case, we can simply extract the 'normal' column from it (as vector) and attach it to one of the others. Let's try:

```{r}

dt = dt_list[[1]]
normal_probs_as_vector = dt_list[[2]][,normal]
dt[,normal := normal_probs_as_vector]

ggplot_dt(dt,'normal')
```

Plotting is usually a great way to see whether data got arranged correctly:  Here, we can be fairly certain it did, simply because the missing values in the 'normal' vector are at the locations where they should be (over water and dry regions). If the ordering would have been differently, these would be all over the place. However, let's run another test to be certain:

```{r}
# attach the 'above'-data table:
dt = merge(dt,dt_list[[3]],by = c('lon','lat'))
print(dt)

# if the ordering of the 'normal' column was correct, we have below + normal + above = 100%:
check = rowSums(dt[,.(below,normal,above)])
print(check[1:20])
mean(check[!is.na(check)])
```

We were lucky and the ordering was correct. Is there anything we could have done otherwise? Well, yes, we could have just used $\text{below} + \text{normal} + \text{above} = 100\%$ right away:

```{r}
# only extract 'below' and 'above':
dt = netcdf_to_dt(paste0(data_dir,fn), vars = c('below','above'),verbose = 0)
print(dt)
dt[,normal := 100 - below - above]
ggplot_dt(dt,'normal')
```




### Example: Upscaling observations {#us-obs}

Here we prepare a dataset for evaluating tercile forecasts for the MAM season: In our example data directory `data_dir` (given above) there are three datasets we need to combine to this end. Predictions, past observations and the 2021-observation. Note that we require past observations in order to find the climatology terciles, so that we can check whether the observed rainfall at a gridpoint is indeed 'high' or 'low' for that gridpoint.,
Our main challenge is that the 2021-observation file looks quite different from the others. In particular it is on a grid with higher resolution. 

```{r,cache = TRUE }
# get predictions:
dt = netcdf_to_dt(paste0(data_dir,'PredictedProbabilityRain_Mar-May_Feb2021_new.nc'))
print(dt)

# past observations:
dt_obs = netcdf_to_dt(paste0(data_dir,'ObservedRain_Mar-May_Feb2021.nc'))
# 2021 observation:
dt_obs2021 = netcdf_to_dt(paste0(data_dir,'ObservedChirpsRainTotal_MAM2021.nc'),vars = 'precip')

# the 2021 observation is named differently...
setnames(dt_obs2021,c('longitude','latitude','precip'),c('lon','lat','prec'))
# ... and it is on higher resolution:
ggplot_dt(dt_obs2021,'prec',high = 'blue',midpoint = 0)

# we can upscale it to half-degree-resolution using the following function:
dt_obs2021 = upscale_to_half_degrees(dt_obs2021,uscol = 'prec',bycols = 'time')
ggplot_dt(dt_obs2021,high = 'blue',midpoint = 0)

# the time format is different for the two observation data tables, see netcdf description above.
# For dt_obs the format is month since date, and can be changed to year-month like this:
dt_obs = MSD_to_YM(dt_obs)
# For dt_obs2021 it's the number of days since 1980-01-01, and we can do the following:
dt_obs2021[,date := as.Date(time,origin = '1980-01-01')] # see netcdf description above
dt_obs2021[,year := year(date)][,month := month(date)]
print(dt_obs2021)

# Next, let's restrict 2021-observations to locations that are not blanked out in the past observations:
na_locs = dt_obs[year == year[1],.(lon,lat,is.na(prec))]
print(na_locs)
dt_obs2021 = merge(dt_obs2021,na_locs,by = c('lon','lat'))
dt_obs2021 = dt_obs2021[!(V3)] # only keep rows for which V3 is FALSE
ggplot_dt(dt_obs2021, 'prec',high = 'blue',midpoint = 0)

#delete what we don't need and bind together:
dt_obs[,month:=NULL]
dt_obs2021[,c('month','time','date','V3'):=NULL]

dt_obs = rbindlist(list(dt_obs,dt_obs2021),use.names = TRUE)
dt_obs = dt_obs[!is.na(prec)]

# in which climatology tercile lies the observation for which year?
dt_obs = add_tercile_cat(dt_obs) 
# let's also add the climatology for alter use:
dt_obs[,clim := mean(prec),by = .(lon,lat)]


ggplot_dt(dt_obs[year == 2021],'tercile_cat',low = 'red',high = 'blue')

# merge prediction and corresponding observation:
dt = merge(dt,dt_obs[year == 2021],by = c('lon','lat'))
# transform percentage prediction to probabilities between zero and one:
dt[,normal := normal/100]
dt[,above := above/100]
dt[,below := below/100]

print(dt)
```

How to evaluate this dataset will be discussed in Section \@ref(eval-terciles).
```{r,echo = F}
dt_tercile_forecast = copy(dt)
```

Besides the function `upscale_to_half_degrees` there is another function called `upscale_nested_griddings` that is more general purpose and makes weaker assumptions about the grids, 



### Example: preparing data for evaluating exceedence probabilities{#data-ex-prexc}

Here we show how to prepare data for evaluating exceedence probabilities, see Section \@ref(eval-ex-pr).

```{r}
fn = 'PrecRegPeXcd_3monthSeasonal.nc'
dt = netcdf_to_dt(paste0(data_dir,fn))
print(dt)

# first, note that the 'model', 'rthr', and 'month' column do not make much sense before we insert
# the information we gather from the netcdf description.:
modelnames = c('GEM-NEMO',
               'CanCM4i',
               'NASA-GEOSS2S',
               'GFDL-SPEAR',
               'COLA-RSMAS-CCSM4',
               'NCEP-CFSv2',
               'ECMWF',
               'Meteo_France',
               'UKMO')
thresholds = c(200,300,350,400)

dt[,model := modelnames[model + 1]]
dt[,rthr := thresholds[rthr + 1]]
dt[,month :=lead + 2][,lead:=NULL]
```

Ultimately, we want to compare the skill of these models to a climatological forecast. The climatological forecast for the exceedence probability is just the fraction of observed years where the threshold was exceeded. To calculate this, we require past observations. These are not contained in our example data folder, so we cheat a little here and load in some chirps data. You can get this data (on high resolution) from here: http://digilib.icpac.net/SOURCES/.ICPAC/.CHIRPS-BLENDED/.monthly/.rainfall/.precipitation/. It can then be upscaled as shown in the last section. Our observation data looks like this:

```{r,echo = FALSE}
dt_chirps = PostProcessing::load_chirps(chirps_fn)[,index_coarse:= NULL]
dt_chirps = dt_chirps[ month >=2 & month <= 5]
dt_chirps[,prec:=30*prec]
```

```{r}
print(dt_chirps)

```

In order to get the climatological exceedence probabilities, we can use the following function:
```{r}
clim_fc = climatology_threshold_exceedence(dt_chirps,
                                           obs_col = 'prec',
                                           thresholds = unique(dt[,rthr]),
                                           by_cols = c('month','lon','lat'))

print(clim_fc)
```

Note that we passed the thresholds given in `dt`. The `bycols` argument tells the function what columns to group by when computing the climatology. Finally, we need to merge the predictions, the climatological forecast and the observation into one data table. Since we only have predictions for 2021, it is enough to provide the climatology forecast and observation for 2021 as well. Also note that we have predictions for more months than observations (at the time this is written), so we cut the predictions for June and July out - we cannot evaluate predictions we don't know the outcome for.

```{r}
setnames(clim_fc,c('pexcd','threshold'),c('clim','rthr'))
dt = merge(dt,clim_fc[year == 2021,],by = c('lon','lat','month','rthr'))
dt = merge(dt,dt_chirps[year == 2021],by = c('lon','lat','month','year'))

#finally, for evaluation we generally work with probabilities between 0 and 1, not percentages:
range(dt[,pexcd],na.rm = TRUE) # confirm that the data table contains percentages at the moment...
dt[,pexcd := pexcd/100] #... and correct

print(dt)
```
How to evaluate the predictions from here is discussed in Section \@ref(eval-ex-pr).

```{r,echo = FALSE}
dt_prexc = dt
save(dt_tercile_forecast,dt_cv,dt_obs,dt_prexc,file = '_temp.RData')
```
