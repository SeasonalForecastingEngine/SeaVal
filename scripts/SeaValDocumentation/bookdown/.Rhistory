}
return(Score_dt)
}
climatology_threshold_exceedence = function(obs_dt,
obs_col = 'prec',
by_cols = intersect(c('lon','lat','month','season'),names(obs_dt)),
thresholds = c(200,300,350,400))
{
clim_dt = climatology_ens_forecast(obs_dt,by_cols = by_cols)
ret_dt = data.table()
for(thr in thresholds)
{
thr_dt = clim_dt[,.(pexcd = mean(get(obs_col) > thr)),by = c(by_cols,'year')]
thr_dt[,threshold := thr]
ret_dt = rbindlist(list(ret_dt,thr_dt))
}
return(ret_dt)
}
#' Calculate exceedence Brier skill score
#'
#' @param fc_dt Data table containing the predictions.
#' @param fc_col column name of the prediction. Contains predicted probabilities of exceedence
#' @param threshold_col which column contains the exceedence threshold?
#' @param obs_col column name of the observations, that is
#' @param by_cols column names of grouping variables, all of which need to be columns in fc_dt.
#' Default is to group by all instances of month, season, lon, lat, system and lead_time that are columns in fc_dt.
#' @export
BSS_ex_dt = function(fc_dt,
fc_col,
clim_col = 'clim',
threshold_col,
obs_col = 'obs',
by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)))
{
es_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = fc_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
clim_es_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = clim_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
setnames(clim_es_dt,'BS_ex','clim_BS_ex')
score_dt = merge(es_dt,clim_es_dt,by = intersect(names(es_dt),names(clim_es_dt)))
score_dt[,BSS_ex := (clim_BS_ex - BS_ex)/clim_BS_ex ]
#deal with divisions by 0:
es_dt[clim_BS_ex == 0 & BS_ex == 0, BSS_ex := 0] # when both are perfect, skill score should be 0
es_dt[clim_BS_ex == 0 & BS_ex > 0, BSS_ex := min(min(BSS_ex,na.rm = T), -1)] # set to something negative (capped at -1), when climatology predicts perfectly and the forecast does not
#(should actually -Inf, but that's bad for plotting).
return(es_dt)
}
clim_fc = climatology_threshold_exceedence(dt_obs,
obs_col = 'prec',
thresholds = unique(dt[,rthr]),
by_cols = c('month','lon','lat'))
print(clim_fc)
setnames(clim_fc,c('pexcd','threshold'),c('clim','rthr'))
dt = merge(dt,clim_fc[year == 2021,],by = c('lon','lat','month','rthr'))
dt = merge(dt,dt_obs[year == 2021],by = c('lon','lat','month','year'))
print(dt)
ggplot_dt(dt[month == 2 & model == 'GEM-NEMO' & rthr == 200],'pexcd')
ggplot_dt(dt[month == 2 & model == 'GEM-NEMO' & rthr == 200],'prec')
dt[,pexcd := pexcd/100]
bss_dt = BSS_ex_dt(dt,fc_col = 'pexcd',threshold_col = 'rthr',obs_col = 'prec',by_cols = c('model','month','lon','lat'))
BS_ex_dt = BS_ex_dt(dt,'pexcd','rthr',obs_col = 'prec',by_cols = c('model','month','lon','lat'))
plot_list = list()
for(mod in unique(bss_dt[,model]))
{
plot_list = c(plot_list,list(ggplot_dt(bss_dt[model == mod& month == 3 & rthr == 200],'BSS_ex',mn = mod, high = 'red',midpoint = 0,rr= c(-1,1))))
}
fc_dt = dt
fc_col = 'pexcd'
threshold_col = 'rthr'
obs_col = 'prec'
by_cols = c('model','month','lon','lat')
BS_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = fc_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
BS_ex_dt = function(fc_dt,fc_col,threshold_col,
obs_col = 'obs',
by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)),
along_cols = c('year'))
{
if(range(fc_dt[,get(fc_col)],na.rm = T)[2] > 1) stop('your predictions should be values between 0 and 1.')
if(is.logical(fc_dt[,get(obs_col)]))
{
Score_dt = fc_dt[,.(BS_ex = (get(fc_col)-get(obs_col))^2),by = c(by_cols,threshold_col)]# the by-arguments are for keeping these columns only
}
if(!is.logical(fc_dt[,get(obs_col)]))
{
Score_dt = fc_dt[,.(BS_ex = (get(fc_col) - (get(obs_col) > get(threshold_col)))^2),by = c(by_cols,threshold_col)]
}
return(Score_dt)
}
BS_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = fc_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
clim_es_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = clim_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
setnames(clim_es_dt,'BS_ex','clim_BS_ex')
score_dt = merge(es_dt,clim_es_dt,by = intersect(names(es_dt),names(clim_es_dt)))
score_dt[,BSS_ex := (clim_BS_ex - BS_ex)/clim_BS_ex ]
score_dt
score_dt = merge(BS_dt,clim_es_dt,by = intersect(names(es_dt),names(clim_es_dt)))
score_dt[,BSS_ex := (clim_BS_ex - BS_ex)/clim_BS_ex ]
#deal with divisions by 0:
es_dt[clim_BS_ex == 0 & BS_ex == 0, BSS_ex := 0] # when both are perfect, skill score should be 0
#deal with divisions by 0:
score_dt[clim_BS_ex == 0 & BS_ex == 0, BSS_ex := 0] # when both are perfect, skill score should be 0
score_dt[clim_BS_ex == 0 & BS_ex > 0, BSS_ex := min(min(BSS_ex,na.rm = T), -1)] # set to something negative (capped at -1), when climatology predicts perfectly and the forecast does not
BSS_ex_dt = function(fc_dt,
fc_col,
clim_col = 'clim',
threshold_col,
obs_col = 'obs',
by_cols = intersect(c('month','season','lon','lat','system','lead_time'),names(fc_dt)))
{
BS_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = fc_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
clim_es_dt = BS_ex_dt(fc_dt = fc_dt,
fc_col = clim_col,
threshold_col = threshold_col,
obs_col = obs_col,
by_cols = by_cols)
setnames(clim_es_dt,'BS_ex','clim_BS_ex')
score_dt = merge(BS_dt,clim_es_dt,by = intersect(names(es_dt),names(clim_es_dt)))
score_dt[,BSS_ex := (clim_BS_ex - BS_ex)/clim_BS_ex ]
#deal with divisions by 0:
score_dt[clim_BS_ex == 0 & BS_ex == 0, BSS_ex := 0] # when both are perfect, skill score should be 0
score_dt[clim_BS_ex == 0 & BS_ex > 0, BSS_ex := min(min(BSS_ex,na.rm = T), -1)] # set to something negative (capped at -1), when climatology predicts perfectly and the forecast does not
#(should actually -Inf, but that's bad for plotting).
return(score_dt)
}
bss_dt = BSS_ex_dt(dt,fc_col = 'pexcd',threshold_col = 'rthr',obs_col = 'prec',by_cols = c('model','month','lon','lat'))
plot_list = list()
for(mod in unique(bss_dt[,model]))
{
plot_list = c(plot_list,list(ggplot_dt(bss_dt[model == mod& month == 3 & rthr == 200],'BSS_ex',mn = mod, high = 'red',midpoint = 0,rr= c(-1,1))))
}
ggpubr::ggarrange(plotlist = plot_list,ncol = 3,nrow = 3)
bss_dt
devtools::document()
q()
devtools::document()
devtools::document()
devtools::document()
q()
### This script renders the SeaVal documentation
# It essentially knits all .rmd files in this folder, starting with index.rmd and then following the naming order (so the file names should start with a number)
### adjust to your setup: ###
setwd('~/pkg/SeaVal/scripts/SeaValDocumentation/bookdown/')
data_dir = '/nr/project/stat/CONFER/Data/validation/example_data/202102/'
output_dir = '/nr/common/www/virtual/files.nr.no/htdocs/samba/CONFER/SeaVal/'
# this directory is shared at http://files.nr.no/samba/CONFER/SeaVal/
save(data_dir,file = '_data_dir.RData') # just for loading it in the bookdown
bookdown::render_book('index.rmd',
'bookdown::gitbook',
new_session = T)
# We need to copy paste, the output_dir option of render_book does not work, see https://github.com/rstudio/bookdown/issues/804
file.copy(list.files('./_book/',full.names = TRUE), to = output_dir, recursive = TRUE)
data("ecmwf_monthly")
#dt[,ens_mean := mean(prec),by = .(lon,lat,year,month)]
df = copy(ecmwf_monthly)
df2020 = df[year == 2020 & month == 10]
ggplot_dt(df2020,'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
library(SeaVal)
data("ecmwf_monthly")
#dt[,ens_mean := mean(prec),by = .(lon,lat,year,month)]
df = copy(ecmwf_monthly)
df2020 = df[year == 2020 & month == 10]
df2020
ggplot_dt(df2020,'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020[member == 1],'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020[member == 2],'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020[member == 3],'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020[member == 4],'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020[member == 5],'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(df2020,'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
dt = df2020
data_col = 'prec'
mn = 'October 2020 ECMWF'
discrete_cs = FALSE
rr = c(1,10)
low = "blue"
mid = "white"
high = "red"
name = data_col
midpoint = NULL
na.value = 'gray50'
oob = scales::squish
guide = guide_colorbar(barwidth = 0.5, barheight = 10)
library(ggplot2)
guide = guide_colorbar(barwidth = 0.5, barheight = 10)
binwidth = NULL
bin_midpoint = midpoint
time_cols = intersect(names(dt),c('month','year','day','date','season'))
dt_sm = dt[,.SD,.SDcols = c('lon','lat',data_col,time_cols)]
dt_sm
dt
dt_sm
?geom_tile
time_cols = intersect(names(dt),c('month','year','day','date','season'))
dt_sm = dt[,.SD,.SDcols = c('lon','lat',data_col,time_cols)]
if(length(time_cols)>0)
{
tc1 = dt_sm[1,.SD,.SDcols = time_cols]
dt_sm = merge(dt_sm,tc1,by = time_cols)
}
world_map <- ggplot2::map_data(map = 'world',resolution = 0)
if(is.null(rr))
{
rr = dt_sm[,range(get(data_col),na.rm = T)]
}
# set midpoint:
if(is.null(midpoint))
{
midpoint = rr[1] + (rr[2]-rr[1])/2
if(is.null(bin_midpoint))
{
bin_midpoint = midpoint
}
}
colorscale = scale_fill_gradient2(low = low, mid = mid, high = high,
name = name, limits = rr, midpoint = midpoint,
na.value = na.value,oob = oob,
guide = guide)
### plotting ###
pp = ggplot(data = dt_sm) +
geom_tile(aes(x = lon,y = lat, fill = get(data_col))) +            # add data plot
geom_polygon(data = world_map,
mapping = aes(x = long,y = lat,group = group),
color = 'black',fill = NA,size=0.25)  +               # add map
colorscale +  # colorscale is specified above
coord_cartesian(xlim = range(dt_sm[,lon],na.rm = T),
ylim = range(dt_sm[,lat],na.rm = T),
expand = FALSE) + # restricts the plot to exactly the considered area to avoid weird borders
#coord_sf(xlim = lon_range,ylim = lat_range,expand = FALSE) +       # restricts the plot to exactly the considered area to avoid weird borders
xlab('') + ylab('') +                                              # remove default labels and background grid...
theme(panel.background = element_rect(fill =na.value), # this is required in case a data table is passed that has 'truely' missing locations, i.e. that is not rectangular
panel.grid = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank())
pp
pp$layers
pp$labels
dt_sm
dt_sm[,.N] > unique(dt_sm[,.(lon,lat)])[,.N]
chirps_monthly
chirps2020 = chirps_monthly[year == 2020 & month == 10]
ggplot_dt(chirps2020,'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
chirps2020
ggplot_dt(chirps2020,'obs',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
?chirps_monthly
prec_dt = ncdf_to_dt(paste0(data_dir,'CHIRPS.nc'))
library(ncdf4)
prec_dt = ncdf_to_dt(paste0(data_dir,'CHIRPS.nc'))
data_dir = '/nr/project/stat/CONFER/Data/'
plot_dir = '/nr/project/stat/CONFER/plots/CHIRPS/'
prec_dt = ncdf_to_dt(paste0(data_dir,'CHIRPS.nc'))
C_dt[,prec := prec/30] # calendar is 360 days, original unit is mm/month
C_dt = prec_dt
setnames(C_dt,c('lon','lat','month','prec'))
C_dt[,prec := prec/30] # calendar is 360 days, original unit is mm/month
C_dt[,year := floor(month/12) + 1960]
C_dt[,month := floor(month)%%12 + 1]
ggplot_dt(C_dt[year == 2020 & month == 10],'prec',rr = c(1,10))
C_dt[year == 2020 & month == 10],'prec',rr = c(1,10)
chirps2020
rm(list = ls)
rm(list = ls())
library(SeaVal)
chirps2020 = chirps_monthly[year == 2020 & month == 10]
ggplot_dt(chirps2020,'obs',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
chirps2020
ggplot_dt(chirps2020,'prec',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
ggplot_dt = function(dt,
data_col = colnames(dt)[3],
mn = NULL,
discrete_cs = FALSE,
rr = NULL,
low = "blue",
mid = "white",
high = "red",
name = data_col,
midpoint = NULL,
na.value = 'gray50',
oob = scales::squish,
guide = guide_colorbar(barwidth = 0.5, barheight = 10),
...,
binwidth = NULL,bin_midpoint = midpoint)
{
####### transform data #######
# if you have spatio-temporal data, plot only only the first time-slice of it (convenient for diagnostics).
time_cols = intersect(names(dt),c('month','year','day','date','season'))
dt_sm = dt[,.SD,.SDcols = c('lon','lat',data_col,time_cols)]
if(length(time_cols)>0)
{
warning('Multiple timepoints detected, the plot shows the data for the first one.')
tc1 = dt_sm[1,.SD,.SDcols = time_cols]
dt_sm = merge(dt_sm,tc1,by = time_cols)
}
if(dt_sm[,.N] > unique(dt_sm[,.(lon,lat)])[,.N])
{
warning('Your data has multiple entries per lon/lat coordinate. By default the last value for each coordinate is plotted.' )
}
#### get map: ####
world_map <- ggplot2::map_data(map = 'world',resolution = 0)
# better maps are available with the rnaturalearth package and can be plotted using geom_sf.
# However, this approach requires gdal, so it's not exactly easily accessible.
#### fix range and set values outside of range to the range border ####
if(is.null(rr))
{
rr = dt_sm[,range(get(data_col),na.rm = T)]
}
# set midpoint:
if(is.null(midpoint))
{
midpoint = rr[1] + (rr[2]-rr[1])/2
if(is.null(bin_midpoint))
{
bin_midpoint = midpoint
}
}
# set colorscale:
if(!discrete_cs)
{
colorscale = scale_fill_gradient2(low = low, mid = mid, high = high,
name = name, limits = rr, midpoint = midpoint,
na.value = na.value,oob = oob,
guide = guide,...)
}
if(discrete_cs)
{
if(!is.null(binwidth))
{
nbinapprox = floor((rr[2] - rr[1])/binwidth)
bins1 = binwidth*(1/2 + (0:nbinapprox)) + bin_midpoint
bins2 = -binwidth*(1/2 + (0:nbinapprox)) + bin_midpoint
bins=  sort(unique(c(bins2,bins1)))
bins = round(bins[bins %between% rr],2)
# for discrete scales there used to be an issue where the boundary bins are shown wider in the legend,
# see https://github.com/tidyverse/ggplot2/issues/4019. This was resolved in ggplot2 version 2.3.4.
colorscale = scale_fill_steps2(low = low, mid = mid, high = high,
name = name, limits = rr, midpoint = midpoint,
breaks = bins,
na.value = na.value, oob = oob, guide = guide,...)
}
if(is.null(binwidth))
{
colorscale = scale_fill_steps2(low = low, mid = mid, high = high,
name = name, limits = rr, midpoint = midpoint,
na.value = na.value, oob = oob,
guide = guide, ...)
}
}
### plotting ###
pp = ggplot(data = dt_sm) +
geom_raster(aes(x = lon,y = lat, fill = get(data_col))) +            # add data plot
geom_polygon(data = world_map,
mapping = aes(x = long,y = lat,group = group),
color = 'black',fill = NA,size=0.25)  +               # add map
colorscale +  # colorscale is specified above
coord_cartesian(xlim = range(dt_sm[,lon],na.rm = T),
ylim = range(dt_sm[,lat],na.rm = T),
expand = FALSE) + # restricts the plot to exactly the considered area to avoid weird borders
#coord_sf(xlim = lon_range,ylim = lat_range,expand = FALSE) +       # restricts the plot to exactly the considered area to avoid weird borders
xlab('') + ylab('') +                                              # remove default labels and background grid...
theme(panel.background = element_rect(fill =na.value), # this is required in case a data table is passed that has 'truely' missing locations, i.e. that is not rectangular
panel.grid = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank())
if(!is.null(mn)) pp = pp + ggtitle(mn)                               # add title, if given
return(pp)
}
ggplot_dt(chirps2020,'prec',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
chirps2020
source('~/pkg/ForecastTools/R/plotting.R')
ggplot_dt(chirps2020,'prec',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
ggplot_dt(ecmwf2020,'prec',
mn = 'October 2020 CHIRPS',
rr = c(1,10),
name = 'mm/day')
data("ecmwf_monthly")
#dt[,ens_mean := mean(prec),by = .(lon,lat,year,month)]
df = copy(ecmwf_monthly)
df2020 = df[year == 2020 & month == 10]
ggplot_dt(df2020,'prec',
mn = 'October 2020 ECWMF',
rr = c(1,10),
name = 'mm/day')
source('~/pkg/ForecastTools/R/plotting.R')
source('~/pkg/ForecastTools/R/plotting.R')
### This script renders the SeaVal documentation
# It essentially knits all .rmd files in this folder, starting with index.rmd and then following the naming order (so the file names should start with a number)
### adjust to your setup: ###
setwd('~/pkg/SeaVal/scripts/SeaValDocumentation/bookdown/')
data_dir = '/nr/project/stat/CONFER/Data/validation/example_data/202102/'
output_dir = '/nr/common/www/virtual/files.nr.no/htdocs/samba/CONFER/SeaVal/'
# this directory is shared at http://files.nr.no/samba/CONFER/SeaVal/
save(data_dir,file = '_data_dir.RData') # just for loading it in the bookdown
bookdown::render_book('index.rmd',
'bookdown::gitbook',
new_session = T)
# We need to copy paste, the output_dir option of render_book does not work, see https://github.com/rstudio/bookdown/issues/804
file.copy(list.files('./_book/',full.names = TRUE), to = output_dir, recursive = TRUE)
library(SeaVal)
library(ggpubr)
library(ggpubr)
library(ggplot2)
load(file = '_temp.RData')
print(dt_cv)
### check out local biases:
bias_dt = dt_cv[,.(bias = mean(prediction - observation)), by = .(lon,lat,season)] # grouping by lon,lat, and season means that the mean is taken over all years.
bias_dt[,range(bias)] # get an idea of the range for plotting
rr = c(-15,15) # fix range, to make plots comparable
pp2 = ggplot_dt(bias_dt[season == 'MAM'],
data_col = 'bias',
rr = rr,
mn = 'bias of MAM prediction',
midpoint = 0)
# show plots:
ggpubr::ggarrange(pp1,pp2)
pp1 = ggplot_dt(bias_dt[season == 'FMA'],
data_col = 'bias',
rr = rr, # fix range to make it comparable to pp2
mn = 'bias of FMA prediction',
midpoint = 0)
pp2 = ggplot_dt(bias_dt[season == 'MAM'],
data_col = 'bias',
rr = rr,
mn = 'bias of MAM prediction',
midpoint = 0)
# show plots:
ggpubr::ggarrange(pp1,pp2)
### analyze mean square error skill scores
msess = MSESS_dt(dt_cv,
fc_col = 'prediction', # column name of forecasts
obs_col = 'observation', # column name of observations
by_cols = c('lon','lat','season')) # the skill scores should be computed for each location and each season separately
fc_dt = dt_cv
fc_col = 'prediction'
fc_dt
obs_col = 'observation'
if(!('year' %in% along_cols)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')
along_cols = c('year')
if(!('year' %in% along_cols)) stop('skill scores are with respect to leave-one-year-out climatology, so your along_cns must contain "year".')
obs_dt = unique(fc_dt[,.SD,.SDcols = intersect(c(obs_col,by_cols,along_cols),c('year','month','season','lon','lat',obs_col))])
by_cols = c('lon','lat','season')
obs_dt = unique(fc_dt[,.SD,.SDcols = intersect(c(obs_col,by_cols,along_cols),c('year','month','season','lon','lat',obs_col))])
obs_dt
obs_by_cols = intersect(by_cols,names(obs_dt))
obs_by_cols
climatology_prediction = climatology_ens_forecast(obs_dt = obs_dt,
by_cols = obs_by_cols)
climatology_prediction
source('~/pkg/SeaVal/R/confer_validation.R')
source('~/pkg/SeaVal/R/confer_validation.R')
q()
