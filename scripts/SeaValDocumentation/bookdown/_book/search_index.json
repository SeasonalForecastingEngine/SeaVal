[["index.html", "Introduction", " The SeaVal package for validating seasonal weather forecasts Claudio Heinrich and Michael Scheuerer, with input from Masilin Gudoshava, Eunice Koech, Anthony Mwanthi, Zewdu Segele, Hussen Seid and Thordis Thorarinsdottir Introduction The R-package SeaVal provides functionality and many useful tools for evaluating seasonal forecasts. It is developed by the Norwegian Computing Center as part of the Horizon 2020 project CONFER. The goal of this project is to improve seasonal weather predictions in east Africa. Some functionality of this package is tailored to this purpose, specifically to evaluation practice and datasets at ICPAC, while other will be useful for evaluating seasonal predictions in general. Evaluating seasonal weather predictions requires an entire pipeline of tasks, including Importing the predictions Downloading and importing corresponding observations Matching predictions and observations, e.g. when they are on different grids Evaluating forecast skill Visualizing and exporting results The SeaVal package provides tools for all of these tasks. The development of this package is supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 869730 (CONFER). "],["getting-started.html", "1 Getting Started 1.1 Installation 1.2 examples of data.table syntax", " 1 Getting Started This tutorial shows how to get started with SeaVal and introduces its main functionality by examples. The package can be downloaded directly from github, see description below. SeaVal relies on R data tables (available with the R package data.table). Data tables are more flexible and memory efficient data frames, and simplify many operations that are frequently required when working with weather- and climate data. An introduction to data tables can be found here. 1.1 Installation In order to directly install the package from github, you need the package devtools. So if you have not installed it yet, you should first run install.packages(&#39;devtools&#39;) Thereafter, the SeaVal package including all dependencies can be installed by running the command devtools::install_github(&#39;SeasonalForecastingEngine/SeaVal&#39;) This may take a while, especially when some of the larger dependency packages are not installed, such as data.table or ggplot2. You may see a message like this: These packages have more recent versions available. It is recommended to update all of them. Which would you like to update? 1: All 2: CRAN packages only 3: None ... In that case type ‘1’ for all. (It is important to not only update CRAN packages: for technical reasons, the SeaVal package depends on a second package ForecastTools hosted on github, which is also developed by the Norwegian Computing Center) If this completes without an error, the setup is complete and you’re good to go. Now, all you have to do is load SeaVal: library(SeaVal) ## Loading required package: data.table ## Loading required package: ForecastTools The rest of the tutorial is organized as follows. We will first give a few examples how to use Rs data table syntax to perform typical tasks required for evaluation. Thereafter, we will show how SeaVal can be used to visualize data as spatial maps. This will be followed by examples how to import forecasts and download observations data. Thereafter, the tutorial shows how observations and predictions can be combined into a suitable dataset for evaluation. Finally, it shows how to evaluate different types of forecasts 1.2 examples of data.table syntax Here, we show some examples of basic operations on data tables. A short but comprehensive introduction to data.tables syntax can be found here. The SeaVal package comes with a few example data sets we will use here, for example monthly mean precipitation over the GHA region for the OND season provided by CHIRPS: data(&quot;chirps_monthly&quot;) print(chirps_monthly) ## lon lat prec month year terc_cat ## 1: 22.0 -12.0 1.9301587 10 1981 -1 ## 2: 22.0 -11.5 2.1351602 10 1981 -1 ## 3: 22.0 -11.0 2.7692883 10 1981 -1 ## 4: 22.0 -10.5 3.9201619 10 1981 0 ## 5: 22.0 -10.0 4.8720656 10 1981 1 ## --- ## 418076: 51.5 21.0 0.2404348 12 2020 -1 ## 418077: 51.5 21.5 0.2184058 12 2020 -1 ## 418078: 51.5 22.0 0.2053623 12 2020 -1 ## 418079: 51.5 22.5 0.1615942 12 2020 -1 ## 418080: 51.5 23.0 0.1387682 12 2020 -1 A short description of the dataset is also provided: ?chirps_monthly We’ll now go over a few basic commands for handling this sort of data. chirps_monthly is a data table, which is an enhanced data frame. The most fundamental operations include subsetting, performing calculations on columns and aggregation or grouping for calculations. Examples for subsetting are chirps_monthly[month == 10] # extract the data for October ## lon lat prec month year terc_cat ## 1: 22.0 -12.0 1.930158724 10 1981 -1 ## 2: 22.0 -11.5 2.135160243 10 1981 -1 ## 3: 22.0 -11.0 2.769288266 10 1981 -1 ## 4: 22.0 -10.5 3.920161870 10 1981 0 ## 5: 22.0 -10.0 4.872065602 10 1981 1 ## --- ## 139356: 51.5 21.0 0.033333333 10 2020 -1 ## 139357: 51.5 21.5 0.033333333 10 2020 0 ## 139358: 51.5 22.0 0.032608688 10 2020 -1 ## 139359: 51.5 22.5 0.001594238 10 2020 -1 ## 139360: 51.5 23.0 0.000000000 10 2020 -1 chirps_monthly[year %between% c(1990,1999)] # extract the data for 1990 - 1999 ## lon lat prec month year terc_cat ## 1: 22.0 -12.0 3.1261905 10 1990 1 ## 2: 22.0 -11.5 3.5520651 10 1990 1 ## 3: 22.0 -11.0 3.9256340 10 1990 1 ## 4: 22.0 -10.5 4.4879379 10 1990 1 ## 5: 22.0 -10.0 4.4143639 10 1990 0 ## --- ## 104516: 51.5 21.0 0.2565218 12 1999 0 ## 104517: 51.5 21.5 0.2427537 12 1999 1 ## 104518: 51.5 22.0 0.2171015 12 1999 1 ## 104519: 51.5 22.5 0.2000000 12 1999 1 ## 104520: 51.5 23.0 0.1981884 12 1999 1 chirps_monthly[1000:2000] # extract rows 1000 - 2000 ## lon lat prec month year terc_cat ## 1: 29 -9.5 1.74899961 10 1981 0 ## 2: 29 -9.0 1.44546648 10 1981 -1 ## 3: 29 -8.5 1.45933371 10 1981 -1 ## 4: 29 -8.0 1.52153314 10 1981 -1 ## 5: 29 -7.5 1.35046587 10 1981 -1 ## --- ## 997: 36 -8.5 0.49439943 10 1981 0 ## 998: 36 -8.0 0.31293185 10 1981 0 ## 999: 36 -7.5 0.06879925 10 1981 -1 ## 1000: 36 -7.0 0.01446661 10 1981 -1 ## 1001: 36 -6.5 0.04019988 10 1981 0 chirps_monthly[month == 10][lon &gt; 30][terc_cat == 0] #chained subsetting: get all October values at locations with longitude &gt;30 that had normal rainfall (terc_cat == 0) ## lon lat prec month year terc_cat ## 1: 30.5 -10.5 0.67720063 10 1981 0 ## 2: 30.5 -10.0 0.78526832 10 1981 0 ## 3: 30.5 -9.5 1.03640187 10 1981 0 ## 4: 30.5 -9.0 0.96939957 10 1981 0 ## 5: 30.5 -8.5 0.52219994 10 1981 0 ## --- ## 32909: 51.0 20.0 0.06666667 10 2020 0 ## 32910: 51.0 21.5 0.03333333 10 2020 0 ## 32911: 51.5 19.5 0.06666667 10 2020 0 ## 32912: 51.5 20.0 0.06666667 10 2020 0 ## 32913: 51.5 21.5 0.03333333 10 2020 0 chirps_monthly[month == 10 &amp; lon &gt; 30 &amp; terc_cat == 0] # different syntax, same effect. ## lon lat prec month year terc_cat ## 1: 30.5 -10.5 0.67720063 10 1981 0 ## 2: 30.5 -10.0 0.78526832 10 1981 0 ## 3: 30.5 -9.5 1.03640187 10 1981 0 ## 4: 30.5 -9.0 0.96939957 10 1981 0 ## 5: 30.5 -8.5 0.52219994 10 1981 0 ## --- ## 32909: 51.0 20.0 0.06666667 10 2020 0 ## 32910: 51.0 21.5 0.03333333 10 2020 0 ## 32911: 51.5 19.5 0.06666667 10 2020 0 ## 32912: 51.5 20.0 0.06666667 10 2020 0 ## 32913: 51.5 21.5 0.03333333 10 2020 0 We can subset either by logical expressions (first two examples) or by row indices (third example). Subsetting always returns a data table, e.g. chirps_monthly[1] returns a one-row data table containing the first row of chirps_monthly. Next, let’s look at examples for operations on columns: chirps_monthly[,mean(prec)] # get the mean precipitation (over all locations, months, years) ## [1] 1.995215 chirps_monthly[,mean_prec := mean(prec)] # create a new column in the data table containing the mean chirps_monthly[,prec := 30*prec] # transform precipitation from unit mm/day to mm (per month) print(chirps_monthly) ## lon lat prec month year terc_cat mean_prec ## 1: 22.0 -12.0 57.904762 10 1981 -1 1.995215 ## 2: 22.0 -11.5 64.054807 10 1981 -1 1.995215 ## 3: 22.0 -11.0 83.078648 10 1981 -1 1.995215 ## 4: 22.0 -10.5 117.604856 10 1981 0 1.995215 ## 5: 22.0 -10.0 146.161968 10 1981 1 1.995215 ## --- ## 418076: 51.5 21.0 7.213044 12 2020 -1 1.995215 ## 418077: 51.5 21.5 6.552173 12 2020 -1 1.995215 ## 418078: 51.5 22.0 6.160868 12 2020 -1 1.995215 ## 418079: 51.5 22.5 4.847827 12 2020 -1 1.995215 ## 418080: 51.5 23.0 4.163046 12 2020 -1 1.995215 Note in all cases the ‘,’ after ‘[’ which tells data table that you’re doing an operation rather than trying to subset. We can also put things together and subset and operate simultaneously. In this case the subsetting is specified first, followed by a comma followed by the operation: chirps_monthly[month == 10 , mean(prec)] # get the mean precipitation for October (over all locations, years) ## [1] 66.30285 (Note that the mean is much larger now because we changed units…). Finally, and most importantly, we can perform operations over aggregated groups: dt_new = chirps_monthly[, mean(prec),by = .(lon,lat,month)] print(dt_new) ## lon lat month V1 ## 1: 22.0 -12.0 10 84.507765 ## 2: 22.0 -11.5 10 92.716755 ## 3: 22.0 -11.0 10 104.027108 ## 4: 22.0 -10.5 10 116.425850 ## 5: 22.0 -10.0 10 128.969907 ## --- ## 10448: 51.5 21.0 12 7.679457 ## 10449: 51.5 21.5 12 7.247609 ## 10450: 51.5 22.0 12 6.454131 ## 10451: 51.5 22.5 12 5.612391 ## 10452: 51.5 23.0 12 5.103941 Here, the ‘by’ command (after the second comma) tells data table to perform the operation (mean) for each unique instance of lon, lat, and month separately. As a result, the mean is taken only over all years, and a separate mean is derived for each location and each month. Therefore, this operation derives the monthly local climatology. As we can see, the output is a data table containing all columns in by and a column named V1 containing the output of the operation. That’s a bit impractical, so let’s rename the last column: setnames(dt_new,&#39;V1&#39;,&#39;clim&#39;) # take the data table from above and rename column &#39;V1&#39; into &#39;clim&#39; It’s also possible to name the column direcly while dt_new is created, like this: dt_new = chirps_monthly[,.(clim = mean(prec)),by = .(lon,lat,month)] # same as above, but with simultaneously setting the name of the new column This can again be combined with subsetting: dt_new = chirps_monthly[year %in% 1990:2020, .(clim = mean(prec)), by = .(lon,lat,month)] # computes climatology based on the years 1990-2020 only. In the examples above we create a new data table containing the climatology. If we instead want to add the climatology as a new column to chirps_monthly directly, we need to use the := operator: chirps_monthly[,clim := mean(prec), by = .(lon,lat,month)] # add the climatology column directly into chirps_monthly. This showcases some of the functionalities and syntax of the data.table package. There’s a lot more to it and we recommend having a look at this introduction to data.table if you are not familiar with data tables. In particular, this introduction explains the logic behind the syntax making it much easier to memorize. We’ll finish this section by an example where we compute the MSE for raw ecmwf forecasts: data(&quot;chirps_monthly&quot;) # reload data to reverse the changes made in the examples above. data(&quot;ecmwf_monthly&quot;) # get example hindcasts from ecmwf print(ecmwf_monthly) ## lon lat year month prec member below normal above ## 1: 22 13.0 1993 10 0.0118023199 1 0.60 0.20 0.20 ## 2: 22 13.0 1993 10 0.0257743523 2 0.60 0.20 0.20 ## 3: 22 13.0 1993 10 0.0008075972 3 0.60 0.20 0.20 ## 4: 22 13.0 1993 10 0.1190863216 4 0.60 0.20 0.20 ## 5: 22 13.0 1993 10 0.0002728474 5 0.60 0.20 0.20 ## --- ## 868556: 51 11.5 2020 12 0.8410410288 1 0.36 0.28 0.36 ## 868557: 51 11.5 2020 12 0.7719732821 2 0.36 0.28 0.36 ## 868558: 51 11.5 2020 12 1.4691380784 3 0.36 0.28 0.36 ## 868559: 51 11.5 2020 12 1.2364573109 4 0.36 0.28 0.36 ## 868560: 51 11.5 2020 12 1.2289135981 5 0.36 0.28 0.36 # merge observations and predictions into a single data table: setnames(chirps_monthly,&#39;prec&#39;,&#39;obs&#39;) # rename the &#39;prec&#39; column in the observation data table to &#39;obs&#39;, # in order to avoid name clashes, since ecmwf_monthly also contains a column &#39;prec&#39;, # containing the predictions for precip. dt = merge(ecmwf_monthly,chirps_monthly, by = c(&#39;lon&#39;,&#39;lat&#39;,&#39;year&#39;,&#39;month&#39;)) # merge hindcasts and observations into one data table. print(dt) ## lon lat year month prec member below normal above obs ## 1: 22 13.0 1993 10 0.0118023199 1 0.60 0.20 0.20 0.3490475 ## 2: 22 13.0 1993 10 0.0257743523 2 0.60 0.20 0.20 0.3490475 ## 3: 22 13.0 1993 10 0.0008075972 3 0.60 0.20 0.20 0.3490475 ## 4: 22 13.0 1993 10 0.1190863216 4 0.60 0.20 0.20 0.3490475 ## 5: 22 13.0 1993 10 0.0002728474 5 0.60 0.20 0.20 0.3490475 ## --- ## 868556: 51 11.5 2020 12 0.8410410288 1 0.36 0.28 0.36 0.5262209 ## 868557: 51 11.5 2020 12 0.7719732821 2 0.36 0.28 0.36 0.5262209 ## 868558: 51 11.5 2020 12 1.4691380784 3 0.36 0.28 0.36 0.5262209 ## 868559: 51 11.5 2020 12 1.2364573109 4 0.36 0.28 0.36 0.5262209 ## 868560: 51 11.5 2020 12 1.2289135981 5 0.36 0.28 0.36 0.5262209 ## terc_cat ## 1: 0 ## 2: 0 ## 3: 0 ## 4: 0 ## 5: 0 ## --- ## 868556: 0 ## 868557: 0 ## 868558: 0 ## 868559: 0 ## 868560: 0 dt[,ens_mean := mean(prec),by = .(lon,lat,year,month)] # get the ensemble mean as a new column. # The mean is here grouped over all dimension variables excluding &#39;member&#39;, # therefore the ensemble mean is returned. In other words, a separate mean # is calculated for every instance of lon, lat, year and month. mse_dt = dt[,.(mse = mean((prec-obs)^2)), by = .(lon,lat,month)] # create a new data.table containing the mse by location and month print(mse_dt) ## lon lat month mse ## 1: 22.0 13.0 10 7.263275e-01 ## 2: 22.0 13.0 11 2.330523e-03 ## 3: 22.0 13.0 12 4.717186e-05 ## 4: 22.5 12.5 10 1.100971e+00 ## 5: 22.5 12.5 11 8.114504e-03 ## --- ## 6200: 51.0 11.0 11 2.520367e+00 ## 6201: 51.0 11.0 12 1.086045e+00 ## 6202: 51.0 11.5 10 1.304131e+00 ## 6203: 51.0 11.5 11 3.431530e+00 ## 6204: 51.0 11.5 12 1.362873e+00 # plot mse for October: ggplot_dt(mse_dt[month == 10],&#39;mse&#39;,rr = c(-10,10) ) The function ggplot_dt is used to create spatial plots from data stored in data tables. The next section highlights how to use this function and how the generated plots can be manipulated. "],["plotting.html", "2 Plotting 2.1 Plotting values for selected countries 2.2 Customized plots", " 2 Plotting The function ggplot_dt takes a data table containing two columns named lon and lat that should specify a regular longitude-latitude grid, as well as some data to use for coloring the map. The naming of the columns is important in that the function will not work if you, for example, name the longitude column long or Lon. An easy example is the following data(&quot;chirps_monthly&quot;) dt = copy(chirps_monthly) # to manipulate the data table: chirps_monthly has locked binding dt2020 = dt[year == 2020 &amp; month == 10] # reduce the observed precipitation data to a single time slice, namely October 2020 # our data now looks like this: print(dt2020) ## lon lat prec month year terc_cat ## 1: 22.0 -12.0 2.981750252 10 2020 0 ## 2: 22.0 -11.5 3.382063644 10 2020 0 ## 3: 22.0 -11.0 3.250394658 10 2020 0 ## 4: 22.0 -10.5 3.065396443 10 2020 -1 ## 5: 22.0 -10.0 3.416906726 10 2020 -1 ## --- ## 3480: 51.5 21.0 0.033333333 10 2020 -1 ## 3481: 51.5 21.5 0.033333333 10 2020 0 ## 3482: 51.5 22.0 0.032608688 10 2020 -1 ## 3483: 51.5 22.5 0.001594238 10 2020 -1 ## 3484: 51.5 23.0 0.000000000 10 2020 -1 ggplot_dt(dt2020,&#39;prec&#39;) # we pass the data table and the name of the column containing the plotting data, As we can see, the color scale here makes no sense (blue meaning no precipitation) and we’ll talk about that in a second. But let’s start at the base functionality. ggplot_dt requires two arguments, the first one being a data table containing the data, and the second one is the name of the column that contains the data you want to plot. This defaults to the third column in the data table (often your data table will start with lon, lat, and the third column is what you want to plot). So in the example above, ggplot_dt(dt) would have led to the same result, because 'prec' is the third column in dt. The plotting window is determined by the data. If you have data covering the entire earth, the entire earth would be plotted. As a consequence, we can restrict the plotted region by subsetting the data table: dt_sub = dt2020[lon %between% c(28,43) &amp; lat %between% c(-12,1)] # a region containing Tanzania ggplot_dt(dt_sub) The function has further optional arguments (also recall that you can access the function documentation summarizing all of this by typing ?ggplot_dt): ggplot_dt(dt_sub,&#39;prec&#39;, mn = &#39;October 2020 precipitation&#39;, # add a title to the plot rr = c(1,10), # fix the limits of the color scale name = &#39;mm/day&#39;) # change the legend label In this example we set the lower limit of the color scale to 1 and the upper limit to 10. By default the data is truncated at the ends of the color scale, so every pixel with precipitation of below 1mm/day is now shown in the same blue color, the color corresponding to a value of 1 mm. Setting the range of the color scale is useful for making several plots comparable or to force symmetry around 0, e.g. when plotting correlations or anomalies: dt[,clim := mean(prec), by = .(lon,lat,month)] # add a climatology column to dt dt[,ano := prec - clim] # add an anomaly column to dt print(dt) ## lon lat prec month year terc_cat clim ano ## 1: 22.0 -12.0 1.9301587 10 1981 -1 2.8169255 -0.886766778 ## 2: 22.0 -11.5 2.1351602 10 1981 -1 3.0905585 -0.955398264 ## 3: 22.0 -11.0 2.7692883 10 1981 -1 3.4675703 -0.698281996 ## 4: 22.0 -10.5 3.9201619 10 1981 0 3.8808617 0.039300191 ## 5: 22.0 -10.0 4.8720656 10 1981 1 4.2989969 0.573068714 ## --- ## 418076: 51.5 21.0 0.2404348 12 2020 -1 0.2559819 -0.015547112 ## 418077: 51.5 21.5 0.2184058 12 2020 -1 0.2415870 -0.023181188 ## 418078: 51.5 22.0 0.2053623 12 2020 -1 0.2151377 -0.009775418 ## 418079: 51.5 22.5 0.1615942 12 2020 -1 0.1870797 -0.025485473 ## 418080: 51.5 23.0 0.1387682 12 2020 -1 0.1701314 -0.031363167 ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, rr = c(-3,3)) Now, in this plot positive rainfall anomalies are shown red while negative anomalies are blue, which is very unintuitive. The function allows us to specify the three used colors by name with the arguments low,mid, and high. An overview over available color names can be found here. So here’s an anomaly plot looking a bit nicer: ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, rr = c(-3,3), low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, name = &#39;mm/day&#39;) Note that we set the range argument to c(-3,3). Fixing the range mostly makes sense when the range is known (e.g. for correlation plots), or when you want to compare several plots (e.g. for comparing mean square error of different NWP models, all plots should have the same range). If we leave the range argument rr free, the range is determined from the data. However, when we do this for our anomaly plot this has an undesired sideeffect: ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, name = &#39;mm/day&#39;) The color scale is no longer centered (white) at 0, but in the center of the (now asymetric) range. As a consequence, all gridpoints with anomaly 0 are shown in a light red. To fix this, we can use the midpoint argument: ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) Another, maybe surprising, use for the midpoint argument is that we can generate plots with a colorscale with only two colors. For example, going back to plotting the observed rainfall we can do the following: ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;prec&#39;, mn = &#39;October 2020 rainfall&#39;, mid = &#39;white&#39;, high = &#39;blue&#39;, midpoint = 0, name = &#39;mm/day&#39;) Here, we set the midpoint to 0, which is the minimum of our data (since observed rainfall is never below 0). Consequently, the second half of the colorscale extending below 0 is ignored. Finally, the function allows to discretize the color scale. To this end the argument discrete_cs should be set to TRUE. We can then control the breaks of the discrete colorscale by one of the arguments binwidth, n.breaks, or breaks (the latter takes a vector containing all breakpoints). Using binwidth is recommended: The argument n.breaks (which is passed to the function ggplot2::scale_fill_steps2) tries to find ‘nice’ breaks and does not work reliably, and breaks is often a bit tedious. To revisit the anomaly plot from above: ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, discrete_cs = TRUE, binwidth = 2, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) For saving a created plot, we can use any of Rs graphical devices, e.g. pdf(file = &#39;&lt;path to file and filename&gt;.pdf&#39;, width = ...,height = ...) print(pp) dev.off() This creates a .pdf file, but you can print .png and some other file formats similarly, see ?Devices for an overview. One final remark: Often you will deal with data tables that contain spatio-temporal data. It is then important to remember subselecting the particular timeslice you want to view, (October 2020 in the examples above). The function ggplot_dt by default tries to select the first timeslice of tempo-spatial data. This is convenient for a quick first look at your data. Here an example: print(chirps_monthly) # a data table with multiple months and years and locations, so spatio-temporal data ## lon lat prec month year terc_cat ## 1: 22.0 -12.0 1.9301587 10 1981 -1 ## 2: 22.0 -11.5 2.1351602 10 1981 -1 ## 3: 22.0 -11.0 2.7692883 10 1981 -1 ## 4: 22.0 -10.5 3.9201619 10 1981 0 ## 5: 22.0 -10.0 4.8720656 10 1981 1 ## --- ## 418076: 51.5 21.0 0.2404348 12 2020 -1 ## 418077: 51.5 21.5 0.2184058 12 2020 -1 ## 418078: 51.5 22.0 0.2053623 12 2020 -1 ## 418079: 51.5 22.5 0.1615942 12 2020 -1 ## 418080: 51.5 23.0 0.1387682 12 2020 -1 ggplot_dt(chirps_monthly) # generates a plot of the precipitation of October 1981 (first timeslice), for a first quick impression of your data. 2.1 Plotting values for selected countries Above, we have already seen an option how to restrict a plot to a particular country: by manually subsetting the data to a rectangle of longitudes and latitudes containing that specific country. This is of course quite tedious, and to make our lives easier we can use the restrict_to_country-function that takes a data table and a country name, and subsets the data table to only contain gridpoints in the specified country. Currently, the function accepts the following country names: Burundi, Eritrea, Ethiopia, Kenya, Rwanda, Somalia, South Sudan, Sudan, Tanzania, Uganda. dt_new = restrict_to_country(dt[month == 10 &amp; year == 2020],&#39;Kenya&#39;) print(dt_new) ## lon lat prec month year terc_cat clim ano ## 1: 34.0 -1.0 4.9593979 10 2020 1 2.964475 1.9949232 ## 2: 34.0 -0.5 4.7645891 10 2020 1 3.639693 1.1248956 ## 3: 34.0 0.0 4.6154697 10 2020 1 4.284345 0.3311242 ## 4: 34.5 -1.0 7.5550100 10 2020 1 4.380272 3.1747380 ## 5: 34.5 -0.5 6.4341863 10 2020 1 4.056996 2.3771907 ## --- ## 187: 41.0 -1.0 0.7533339 10 2020 -1 2.456691 -1.7033568 ## 188: 41.0 3.0 1.1354665 10 2020 0 2.135546 -1.0000791 ## 189: 41.0 3.5 1.5012010 10 2020 0 2.191273 -0.6900723 ## 190: 41.0 4.0 1.8609376 10 2020 0 2.431412 -0.5704748 ## 191: 41.5 3.5 1.3543324 10 2020 0 2.447758 -1.0934261 ggplot_dt(dt_new, &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, discrete_cs = TRUE, binwidth = 2, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) As we can see, the function restricts the data to all gridcells for which the centerpoint lies within the specified country. This is useful, for example, for calculating mean scores for the specified country. However, it is not optimal for plotting, since all grid cells past the border are censored, even though the original data table contained values there. To this end, the restrict_to_country function has a rectangle-argument that you can set to TRUE for plotting: dt_new = restrict_to_country(dt[month == 10 &amp; year == 2020],&#39;Kenya&#39;, rectangle = TRUE) ggplot_dt(dt_new, &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, discrete_cs = TRUE, binwidth = 2, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) Instead of a single country name, you can also pass multiple country names in a vector to the function. Moreover, when you use rectangle = TRUE, you can specify a tolerance tol in order to widen the plotting window: dt_new = restrict_to_country(dt[month == 10 &amp; year == 2020], c(&#39;Kenya&#39;,&#39;Tanzania&#39;), rectangle = TRUE,tol = 2) ggplot_dt(dt_new, &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, discrete_cs = TRUE, binwidth = 2, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) The tol = 2 argument means that the function will include a buffer zone of 2 degrees lon/lat outside the specified countries (i.e. 4 gridpoints to each side). Note that the buffer to the south of Tanzania is smaller, because the original data table dt does not contain any data further south. 2.2 Customized plots The function ggplot_dt is, as its name suggests, based on the package ggplot2. This is a widely-used package and there are many books and tutorials available for getting familiar with the syntax, e.g. (this one)[https://ggplot2-book.org/]. In ggplot2, plots are composed out of multiple layers, allowing for successive adding of layers. This can help us to generate highly customized plots. As an example, let’s revisit the anomaly plot from above and add the location of Nairobi an Addis Abbaba to it: library(ggplot2) # get locations as data table: loc = data.table(name = c(&#39;Addis Abbaba&#39;,&#39;Nairobi&#39;),lon = c(38.77,36.84),lat = c(9,-1.28)) print(loc) ## name lon lat ## 1: Addis Abbaba 38.77 9.00 ## 2: Nairobi 36.84 -1.28 pp = ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, mn = &#39;October 2020 rainfall anomaly&#39;, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) + geom_point(data = loc,mapping = aes(x = lon,y = lat)) + geom_text(data = loc,mapping = aes(x = lon,y = lat,label = name),vjust = 1.5) print(pp) Here, we added two layers to the original plot, the first one being the geom_point-layer that creates the two points at the locations of the cities, and the second being the geom_text-layer that labels the points by the city names. A frequently required operation is the changing of the font sizes of title and labels. The easiest way to do this is the command theme_set(theme_bw(base_size = 16)) # defaults to 12 print(pp) We can also use ggplots adding-layer-syntax to overwrite existing layers, for example if we want a fully customized colorscale: library(viridis) # the viridis package contains some nice color scales pp_new = pp + scale_fill_viridis(name = &#39;my_color_scale&#39;, breaks = seq(-5,5,by = 2), guide = guide_colorbar(title = &#39;my personal color scale&#39;, title.position = &#39;top&#39;, barwidth = 20, direction = &#39;horizontal&#39;)) + xlab(&#39;lon&#39;) + ylab(&#39;lat&#39;) + # label axis theme(panel.background = element_rect(fill = &#39;salmon&#39;), # change background color (used for missing values) to something whackey axis.ticks = element_line(), # add ticks... axis.text = element_text(), # ... and labels for the axis, i.e. some lons and lats. legend.position = &#39;bottom&#39;) print(pp_new) For comparing multiple plots (potentially all of them with the same legend), the function ggpubr::ggarrange is useful: library(ggpubr) # compare 2019 October anomaly to 2020 anomaly: rr = c(-5,5) # force color scale to be identical for the plots pp1 = ggplot_dt(dt[month == 10 &amp; year == 2019], &#39;ano&#39;, rr = rr, mn = &#39;October 2019 rainfall anomaly&#39;, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, guide = guide_colorbar(barwidth = 20,barheight = 1,direction = &#39;horizontal&#39;), midpoint = 0, name = &#39;mm/day&#39;) + geom_point(data = loc,mapping = aes(x = lon,y = lat)) + geom_text(data = loc,mapping = aes(x = lon,y = lat,label = name),vjust = 1.5) pp2 = ggplot_dt(dt[month == 10 &amp; year == 2020], &#39;ano&#39;, rr = rr, mn = &#39;October 2020 rainfall anomaly&#39;, low = &#39;red&#39;, mid = &#39;white&#39;, high = &#39;darkgreen&#39;, midpoint = 0, name = &#39;mm/day&#39;) + geom_point(data = loc,mapping = aes(x = lon,y = lat)) + geom_text(data = loc,mapping = aes(x = lon,y = lat,label = name),vjust = 1.5) ggarrange(pp1,pp2,ncol = 2,common.legend = TRUE,legend = &#39;bottom&#39;) "],["data-import-and-processing.html", "3 Data import and processing 3.1 The function netcdf_to_dt 3.2 Downloading and processing CHIRPS data", " 3 Data import and processing The SeaVal package provides some tools for data import and export (currently limited to netcdf files). Moreover, evaluation always requires comparison to observations, and the package downloads and organizes monthly means CHIRPS data. Note that, for seasonal forecasts, observations are frequently considered relative to the local climatology: For example, high rainfall is frequently defined as more rainfall than in 2/3 of all other years (at the samme location and time of year). This requires the download of more observations than just for the year you want to evaluate (because you need to establish what is normal for the considered region/season). 3.1 The function netcdf_to_dt The central function for importing netcdf-data as data.table is called netcdf_to_dt. It takes a filename of a netcdf (including directory path) as argument. The example files we consider are hosted on ICPACs ftp server at SharedData/gcm/seasonal/202102. print(data_dir) # the directory the data is stored in, you need to adjust this to your platform. ## [1] &quot;/nr/project/stat/CONFER/Data/validation/example_data/202102/&quot; fn = &quot;CorrelationSkillRain_Feb-Apr_Feb2021.nc&quot; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/CorrelationSkillRain_Feb-Apr_Feb2021.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float corr[lon,lat] ## lead: 0 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## time: 13 ## _FillValue: -9999 ## ## 3 dimensions: ## time Size:0 *** is unlimited *** (no dimvar) ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ## 6 global attributes: ## units: mm ## MonInit_month: 2 ## valid_time: Feb-Apr ## creation_date: Mon Feb 15 06:59:57 EAT 2021 ## Conventions: None ## title: Correlation between Cross-Validated and Observed Rainfall print(dt) ## lon lat corr ## 1: 20.5 -13.5 NA ## 2: 21.0 -13.5 NA ## 3: 21.5 -13.5 NA ## 4: 22.0 -13.5 NA ## 5: 22.5 -13.5 NA ## --- ## 5078: 51.0 24.5 NA ## 5079: 51.5 24.5 NA ## 5080: 52.0 24.5 NA ## 5081: 52.5 24.5 NA ## 5082: 53.0 24.5 NA By default, the function prints out all the information it gets from the netcdf, including units, array sizes etc. This can be turned off by the verbose argument of the function: setting it to 0 supresses all messages, setting it to 1 only prints units of the variables. The default value is 2. A netcdf file always contains variables (such as precip or temperature) and dimension variables (such as longitude or time). The function netcdf_to_dt by default tries to extract all variables into a single data table that also contains all dimension variables that are indexing at least one variable: For example, the netcdf file above has three dimension variables: lon,lat, and time (which is empty). It has one variable (‘corr’) that is indexed by lon and lat, therefore the resulting data table has three columns: corr, lon and lat. The default behavior of merging all netcdf data into a single data table may sometimes be inappropriate. Say, for example, we have a netcdf with three dimension variables lon,lat and time, and it has a variable precipitation[lon,lat,time] and a second variable grid_point_index[lon,lat]. The resulting data table would have the columns lon,lat,time,precipitation, and grid_point_index. This is not very memory efficient because the grid_point_indices are repeated for every instance of time. Moreover, in this case we probably don’t need the grid_point_index anyway. We can use the vars argument of the netcdf_to_dt function to extract only selected variables. So, in this example, netcdf_to_dt('example_file.nc', vars = 'precipitation') would have done the trick. Merging the data tables for all variables is particularly memory efficient when you have multiple variables that have different dimension variables. For large netcdfs with many variables and many dimension variables this can easily get out of hand. In this case you can use netcdf_to_dt('example_file.nc',trymerge = FALSE). This will return a list of data tables, one data table for each variable, containing only the variable values and the dimension variables it is indexed by. If you have two or more variables that do not share a dimension variable, the function requires you to set trymerge = FALSE, see the example in Section 4.1.2. For the example above, the resulting data table looks like this: ggplot_dt(dt, mn = &#39;Corr. skill rain Feb-Apr, Feb initialized&#39;, # title rr = c(-1,1), # range of the colorbar discrete_cs = TRUE,binwidth = 0.4) # discretize colorbar Note that the area shown by ggplot_dt is always the full extend of the data contained in the data table. In particular, the correlation plot above extends beyond areas where we have data, because the netcdf-file contained these locations (with missing values in the ‘corr’-array). To just plot a window taylored to the data that is not missing, we can simply suppress the missing values by using dt[!is.na(corr)]. We can compare to the February initialized forecast for March to May: fn = &quot;CorrelationSkillRain_Mar-May_Feb2021.nc&quot; dt = netcdf_to_dt(paste0(data_dir,fn),verbose = 0) ggplot_dt(dt[!is.na(corr)], # here we suppress missing values mn = &#39;Corr. skill rain Mar-May, Mar initialized&#39;, # title rr = c(-1,1), # range of the colorbar discrete_cs = TRUE,binwidth = 0.4) # discretize colorbar Similarly, for writing netcdf files from data tables, the package has a function dt_to_netcdf. The function requires a data table as input as well as the names of the columns containing the variables and dimension variables, and a filename to write to. The function will prompt you for units for all variables, but otherwise does not allow to include detailed descriptions in the netcdf. It also currently does not support writing netcdfs with multiple variables that have different dimension variables. You can use the Rpackage ncdf4 for that. 3.2 Downloading and processing CHIRPS data Evaluation of forecasts always requires observations to assess the forecast performance. Moreover, usually we are interested whether the prediction was as good or better than a naive climatological forecast. This requires establishing a climatology which requires access to past observations as well. To this end, the SeaVal package provides code that simplifies the download and use of the CHIRPS monthly means rainfall data set. The CHIRPS data is created by the Climate Hazard Group of UC Santa Barbara. The data is mirrored on the IRI data library, which allows downloading (area-)subsetted data. In order to download all available CHIRPS monthly mean data to your local machine, it is sufficient to run the function download_chirps_monthly() The first time you run this function, it will ask you to specify a data directory on your local machine. This path is saved and from now on generally used by the SeaVal package for storing and loading data. You can later look up which data directory you specified by typing data_dir() The download_chirps_monthly function comes with several useful options. You can specify months and years you want to download (the default is to download everything there is). Moreover, the function automatically looks up which months have been downloaded previously and only loads the data for months that you are still missing. If you want to re-download and overwrite existing files, you can set update = FALSE. The CHIRPS data is on the very high spatial resolution of 0.05 degree lon/lat. While this makes for great-looking plots, it also means that the entire CHIRPS data is roughly 800 MB on disk, even though it is just monthly means. Moreover loading and processing this data can take a long time. To avoid this, the function provides you options to derive an upscaled version with a coarser spatial resolution (default is 0.5 degree lon/lat). The three possible options are resolution = 'both': This downloads the original data and additionally derives an upscaled version that is easier to work with. This is recommended when you have a bit over 800 MB of disk space to spare. resolution = 'low': Downloads the file and upscales it before saving. Only the coarse resolution is kept. In this format, the entire data is roughly 8 MB on disk. resolution = 'high': Downloads only the original data, and does not upscale. You need roughly 800 MB. By default, the function downloads only data for the greater-horn-of-Africa area. You can change this setting to download larger datasets such as Africa or even global, see function documentation, but be wary of long download times and disk storage. After having downloaded the chirps data, you can load it using the function load_chirps: dt = load_chirps() print(dt) ## lon lat prec year month ## 1: 21.5 -12.0 7.33704274 1981 1 ## 2: 22.0 -12.0 6.98149261 1981 1 ## 3: 22.5 -12.0 7.30148010 1981 1 ## 4: 23.0 -12.0 9.03189596 1981 1 ## 5: 23.5 -12.0 9.07711182 1981 1 ## --- ## 2105106: 49.5 22.5 0.01895365 2022 1 ## 2105107: 50.0 22.5 0.02897875 2022 1 ## 2105108: 50.5 22.5 0.03328009 2022 1 ## 2105109: 51.0 22.5 0.03312253 2022 1 ## 2105110: 51.5 22.5 0.03632925 2022 1 # example plot pp = ggplot_dt(dt[year == 2022 &amp; month == 1],high = &#39;blue&#39;,midpoint = 0) plot(pp) By default, the upscaled data is loaded (which is smaller in memory and loads faster) if it is available. Moreover, the function provides options to only load subsets of the data, and to load the data in the original high resolution (if you kept it by setting resolution = 'both' in download_chirps()): dt = load_chirps(years = 2022,months = 1,us = FALSE) print(dt) ## lon lat prec year month ## 1: 21.50 22.475 0.010631205 2022 1 ## 2: 21.55 22.475 0.010624977 2022 1 ## 3: 21.60 22.475 0.010612711 2022 1 ## 4: 21.65 22.475 0.010597722 2022 1 ## 5: 21.70 22.475 0.005510498 2022 1 ## --- ## 414686: 51.30 -11.975 NA 2022 1 ## 414687: 51.35 -11.975 NA 2022 1 ## 414688: 51.40 -11.975 NA 2022 1 ## 414689: 51.45 -11.975 NA 2022 1 ## 414690: 51.50 -11.975 NA 2022 1 # example plot pp = ggplot_dt(dt,high = &#39;blue&#39;,midpoint = 0) plot(pp) ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. "],["data-import-and-processing-1.html", "4 Data import and processing 4.1 Reshaping data", " 4 Data import and processing 4.1 Reshaping data For forecast validation, the ideal data format is to have all your fore- and hindcasts in the same data table, alongside the corresponding observations. So one column of forecasts, one column of observations and several columns of dimension variables (e.g. year, month, lon,lat). However, this is rarely how your netcdf-data looks like: you’ll often have different netcdfs for observations and fore-/hindcasts. They might, moreover have different units, different missing values, different variable names etc. So to get your data into the preferred shape, you either need to manipulate the netcdf files beforehand, to get exactly the data table you want from netcdf_to_dt, or you can extract several data tables and do the required manipulations in R, using SeaVal and data.table. In this section we show a few examples for this. 4.1.1 Example: cross-validation data In our example folder (see above) we have two crossvalidation datasets and corresponding observations (one for FMA, one for MAM). We will process them simultaneously here, merging everything into one single data table. This is not really making things easier and not generally recommended. It is a great way for us to highlight more data.table-syntax, though. # get the two CV-files: fn_pred1 = &quot;CrossValidatedPredictedRain_Feb-Apr_Feb2021.nc&quot; fn_pred2 = &quot;CrossValidatedPredictedRain_Mar-May_Feb2021.nc&quot; dt_pred1 = netcdf_to_dt(paste0(data_dir,fn_pred1),verbose = 0) # they look the same, we can just look at the information from one of them: dt_pred2 = netcdf_to_dt(paste0(data_dir,fn_pred2)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/CrossValidatedPredictedRain_Mar-May_Feb2021.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float prec[lon,lat,time] ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 1 ## _FillValue: -9999 ## ## 3 dimensions: ## time Size:35 *** is unlimited *** ## units: months since 1981-01-01 ## calendar: standard ## _FillValue: 9.96920996838687e+36 ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ## 6 global attributes: ## units: mm ## MonInit_month: 2 ## valid_time: Mar-May ## creation_date: Mon Feb 15 06:59:57 EAT 2021 ## Conventions: None ## title: Cross Validated Predicted Rainfall Total (mm) #this is how our data looks now: print(dt_pred1) ## lon lat time prec ## 1: 20.5 -13.5 13 NA ## 2: 21.0 -13.5 13 NA ## 3: 21.5 -13.5 13 NA ## 4: 22.0 -13.5 13 NA ## 5: 22.5 -13.5 13 NA ## --- ## 177866: 51.0 24.5 421 NA ## 177867: 51.5 24.5 421 NA ## 177868: 52.0 24.5 421 NA ## 177869: 52.5 24.5 421 NA ## 177870: 53.0 24.5 421 NA # before joining the two data tables, we should add a column identifying which is which: dt_pred1[,season:= &#39;FMA&#39;] dt_pred2[,season:= &#39;MAM&#39;] # bind together dt_pred = rbindlist(list(dt_pred1,dt_pred2)) print(dt_pred) ## lon lat time prec season ## 1: 20.5 -13.5 13 NA FMA ## 2: 21.0 -13.5 13 NA FMA ## 3: 21.5 -13.5 13 NA FMA ## 4: 22.0 -13.5 13 NA FMA ## 5: 22.5 -13.5 13 NA FMA ## --- ## 355736: 51.0 24.5 421 NA MAM ## 355737: 51.5 24.5 421 NA MAM ## 355738: 52.0 24.5 421 NA MAM ## 355739: 52.5 24.5 421 NA MAM ## 355740: 53.0 24.5 421 NA MAM The function rbindlist binds a list of several data tables into one. This only works if they have the same columns, though, otherwise you need to use merge. # next, get the observations: fn_obs1 = &quot;ObservedRain_Feb-Apr_Feb2021.nc&quot; fn_obs2 = &quot;ObservedRain_Mar-May_Feb2021_update.nc&quot; dt_obs1 = netcdf_to_dt(paste0(data_dir,fn_obs1),verbose = 0) dt_obs2 = netcdf_to_dt(paste0(data_dir,fn_obs2) ) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/ObservedRain_Mar-May_Feb2021_update.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float prec[lon,lat,time] ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## _FillValue: -9999 ## ## 3 dimensions: ## time Size:35 *** is unlimited *** ## units: months since 1981-01-01 ## calendar: 360_day ## _FillValue: 9.96920996838687e+36 ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ## 8 global attributes: ## units: mm ## MonInit_month: 2 ## valid_time: Mar-May ## creation_date: Mon Feb 15 06:59:57 EAT 2021 ## Conventions: None ## title: Observed Rainfall Total (mm) ## history: Tue Feb 16 12:38:58 2021: ncatted -a calendar,time,o,c,360_day /dump/SharedData/gcm/seasonal/202102/ObservedRain_Mar-May_Feb2021.nc /dump/SharedData/gcm/seasonal/202102/ObservedRain_Mar-May_Feb2021_update.nc ## NCO: netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco) dt_obs1[,season := &#39;FMA&#39;] dt_obs2[,season := &#39;MAM&#39;] dt_obs = rbindlist(list(dt_obs1,dt_obs2)) Now we have two data tables, one with predictions and one with observations. We want to join them, but we want to have predictions next to observations, so rbindlist does not work for us here, and we need to use merge. However, we should first make sure that the columns are named appropriately: Currently, both dt_obs and dt_pred have a column named prec. setnames(dt_pred,&#39;prec&#39;,&#39;prediction&#39;) setnames(dt_obs,&#39;prec&#39;,&#39;observation&#39;) dt = merge(dt_pred,dt_obs,by = c(&#39;lon&#39;,&#39;lat&#39;,&#39;time&#39;,&#39;season&#39;)) print(dt) ## lon lat time season prediction observation ## 1: 20.5 -13.5 13 FMA NA NA ## 2: 20.5 -13.5 13 MAM NA NA ## 3: 20.5 -13.5 25 FMA NA NA ## 4: 20.5 -13.5 25 MAM NA NA ## 5: 20.5 -13.5 37 FMA NA NA ## --- ## 355736: 53.0 24.5 397 MAM NA NA ## 355737: 53.0 24.5 409 FMA NA NA ## 355738: 53.0 24.5 409 MAM NA NA ## 355739: 53.0 24.5 421 FMA NA NA ## 355740: 53.0 24.5 421 MAM NA NA # remove all rows with missing predictions: dt = dt[!is.na(prediction)] # convert time from the &#39;months since date&#39; (MSD) format to years and months (YM) dt = MSD_to_YM(dt,origin = &#39;1981-01-01&#39;) # (the origin was documented in the netcdf, see above.) print(dt) ## lon lat season prediction observation year month ## 1: 20.5 -11.5 FMA 316.19452 369.36932 1982 2 ## 2: 20.5 -11.5 MAM 202.94411 208.28058 1982 2 ## 3: 20.5 -11.5 FMA 316.20178 252.47144 1983 2 ## 4: 20.5 -11.5 MAM 205.24921 161.22548 1983 2 ## 5: 20.5 -11.5 FMA 317.43375 267.44031 1984 2 ## --- ## 167330: 51.5 22.5 FMA 25.44651 19.71902 2012 2 ## 167331: 51.5 22.5 FMA 25.59836 27.55773 2013 2 ## 167332: 51.5 22.5 FMA 26.03941 25.14965 2014 2 ## 167333: 51.5 22.5 FMA 26.03053 22.23634 2015 2 ## 167334: 51.5 22.5 FMA 26.00327 34.84376 2016 2 We now have the data table in the shape we want it to be, containing both predictions and observations as one column each. In section 5.1 we show how to evaluate the predictions. 4.1.2 Example: ‘corrupted’ netcdf Data handling can be messy and things can go wrong at any stage. Here, we have a look at a netcdf file where something has gone wrong: fn = &quot;PredictedProbabilityRain_Feb-Apr_Feb2021.nc&quot; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/PredictedProbabilityRain_Feb-Apr_Feb2021.nc (NC_FORMAT_CLASSIC): ## ## 3 variables (excluding dimension variables): ## float below[lon,lat] ## lead: 0 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## _FillValue: -1 ## float normal[ncl4,ncl3] ## _FillValue: -1 ## float above[lon,lat] ## lead: 0 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 2 ## _FillValue: -1 ## ## 5 dimensions: ## time Size:0 *** is unlimited *** (no dimvar) ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ncl3 Size:77 (no dimvar) ## ncl4 Size:66 (no dimvar) ## ## 6 global attributes: ## units: mm ## MonInit_month: 2 ## valid_time: Mar-May ## creation_date: Mon Feb 15 06:59:57 EAT 2021 ## Conventions: None ## title: Predicted Tercile probability ## Error in netcdf_to_dt(paste0(data_dir, fn)): Your file has variables with disjoint dimensions, which should not be stored in a single data table. Either set trymerge to FALSE or select variables with overlapping dimensions in vars. The netcdf_to_dt function prints out the netcdf information, and then crashes with the error message above, saying that we have disjoint dimension variables for some variables. Indeed, looking at the printed out netcdf-description, we have three variables (below,normal,above), and while ‘below’ and ‘above’ are indexed by ‘lon’ and ‘lat’, ‘normal’ is indexed by ‘ncl3’ and ‘ncl4’. As the error message suggests, we can set trymerge to FALSE, making netcdf_to_dt return a list of data tables. dt_list = netcdf_to_dt(paste0(data_dir,fn),trymerge = FALSE,verbose = 0) print(dt_list) ## [[1]] ## lon lat below ## 1: 20.5 -13.5 NA ## 2: 21.0 -13.5 NA ## 3: 21.5 -13.5 NA ## 4: 22.0 -13.5 NA ## 5: 22.5 -13.5 NA ## --- ## 5078: 51.0 24.5 NA ## 5079: 51.5 24.5 NA ## 5080: 52.0 24.5 NA ## 5081: 52.5 24.5 NA ## 5082: 53.0 24.5 NA ## ## [[2]] ## ncl4 ncl3 normal ## 1: 1 1 NA ## 2: 2 1 NA ## 3: 3 1 NA ## 4: 4 1 NA ## 5: 5 1 NA ## --- ## 5078: 62 77 NA ## 5079: 63 77 NA ## 5080: 64 77 NA ## 5081: 65 77 NA ## 5082: 66 77 NA ## ## [[3]] ## lon lat above ## 1: 20.5 -13.5 NA ## 2: 21.0 -13.5 NA ## 3: 21.5 -13.5 NA ## 4: 22.0 -13.5 NA ## 5: 22.5 -13.5 NA ## --- ## 5078: 51.0 24.5 NA ## 5079: 51.5 24.5 NA ## 5080: 52.0 24.5 NA ## 5081: 52.5 24.5 NA ## 5082: 53.0 24.5 NA We see that ‘ncl3’ and ‘ncl4’ have different values than ‘lon’ and ‘lat’, apparently they are meaningless indexing integers. However, the three data.tables are of the same size, and we can hope that the ‘below’ data table is arranged in the same row-ordering than the others. If this is the case, we can simply extract the ‘normal’ column from it (as vector) and attach it to one of the others. Let’s try: dt = dt_list[[1]] normal_probs_as_vector = dt_list[[2]][,normal] dt[,normal := normal_probs_as_vector] ggplot_dt(dt,&#39;normal&#39;) Plotting is usually a great way to see whether data got arranged correctly: Here, we can be fairly certain it did, simply because the missing values in the ‘normal’ vector are at the locations where they should be (over water and dry regions). If the ordering would have been differently, these would be all over the place. However, let’s run another test to be certain: # attach the &#39;above&#39;-data table: dt = merge(dt,dt_list[[3]],by = c(&#39;lon&#39;,&#39;lat&#39;)) print(dt) ## lon lat below normal above ## 1: 20.5 -13.5 NA NA NA ## 2: 20.5 -13.0 NA NA NA ## 3: 20.5 -12.5 NA NA NA ## 4: 20.5 -12.0 NA NA NA ## 5: 20.5 -11.5 31.13112 35.07441 33.79448 ## --- ## 5078: 53.0 22.5 NA NA NA ## 5079: 53.0 23.0 NA NA NA ## 5080: 53.0 23.5 NA NA NA ## 5081: 53.0 24.0 NA NA NA ## 5082: 53.0 24.5 NA NA NA # if the ordering of the &#39;normal&#39; column was correct, we have below + normal + above = 100%: check = rowSums(dt[,.(below,normal,above)]) print(check[1:20]) ## [1] NA NA NA NA 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 ## [20] 100 mean(check[!is.na(check)]) ## [1] 100 We were lucky and the ordering was correct. Is there anything we could have done otherwise? Well, yes, we could have just used \\(\\text{below} + \\text{normal} + \\text{above} = 100\\%\\) right away: # only extract &#39;below&#39; and &#39;above&#39;: dt = netcdf_to_dt(paste0(data_dir,fn), vars = c(&#39;below&#39;,&#39;above&#39;),verbose = 0) print(dt) ## lon lat below above ## 1: 20.5 -13.5 NA NA ## 2: 20.5 -13.0 NA NA ## 3: 20.5 -12.5 NA NA ## 4: 20.5 -12.0 NA NA ## 5: 20.5 -11.5 31.13112 33.79448 ## --- ## 5078: 53.0 22.5 NA NA ## 5079: 53.0 23.0 NA NA ## 5080: 53.0 23.5 NA NA ## 5081: 53.0 24.0 NA NA ## 5082: 53.0 24.5 NA NA dt[,normal := 100 - below - above] ggplot_dt(dt,&#39;normal&#39;) 4.1.3 Example: Upscaling observations Here we prepare a dataset for evaluating tercile forecasts for the MAM season: In our example data directory data_dir (given above) there are three datasets we need to combine to this end. Predictions, past observations and the 2021-observation. Note that we require past observations in order to find the climatology terciles, so that we can check whether the observed rainfall at a gridpoint is indeed ‘high’ or ‘low’ for that gridpoint., Our main challenge is that the 2021-observation file looks quite different from the others. In particular it is on a grid with higher resolution. # get predictions: dt = netcdf_to_dt(paste0(data_dir,&#39;PredictedProbabilityRain_Mar-May_Feb2021_new.nc&#39;)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/PredictedProbabilityRain_Mar-May_Feb2021_new.nc (NC_FORMAT_NETCDF4): ## ## 3 variables (excluding dimension variables): ## float normal[lon,lat] (Contiguous storage) ## _FillValue: -1 ## float above[lon,lat] (Contiguous storage) ## _FillValue: -1 ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 2 ## float below[lon,lat] (Contiguous storage) ## _FillValue: -1 ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## ## 2 dimensions: ## lat Size:77 ## _FillValue: NaN ## units: degrees_north ## lon Size:66 ## _FillValue: NaN ## units: degrees_east print(dt) ## lon lat normal above below ## 1: 20.5 -13.5 NA NA NA ## 2: 20.5 -13.0 NA NA NA ## 3: 20.5 -12.5 NA NA NA ## 4: 20.5 -12.0 NA NA NA ## 5: 20.5 -11.5 34.11535 33.56262 32.32204 ## --- ## 5078: 53.0 22.5 NA NA NA ## 5079: 53.0 23.0 NA NA NA ## 5080: 53.0 23.5 NA NA NA ## 5081: 53.0 24.0 NA NA NA ## 5082: 53.0 24.5 NA NA NA # past observations: dt_obs = netcdf_to_dt(paste0(data_dir,&#39;ObservedRain_Mar-May_Feb2021.nc&#39;)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/ObservedRain_Mar-May_Feb2021.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float prec[lon,lat,time] ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## _FillValue: -9999 ## ## 3 dimensions: ## time Size:35 *** is unlimited *** ## units: months since 1981-01-01 ## calendar: standard ## _FillValue: 9.96920996838687e+36 ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ## 6 global attributes: ## units: mm ## MonInit_month: 2 ## valid_time: Mar-May ## creation_date: Mon Feb 15 06:59:57 EAT 2021 ## Conventions: None ## title: Observed Rainfall Total (mm) # 2021 observation: dt_obs2021 = netcdf_to_dt(paste0(data_dir,&#39;ObservedChirpsRainTotal_MAM2021.nc&#39;),vars = &#39;precip&#39;) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/ObservedChirpsRainTotal_MAM2021.nc (NC_FORMAT_NETCDF4): ## ## 2 variables (excluding dimension variables): ## double time_bnds[bnds,time] (Chunking: [2,1]) ## float precip[longitude,latitude,time] (Chunking: [592,1,1]) ## standard_name: convective precipitation rate ## long_name: Climate Hazards group InfraRed Precipitation with Stations ## units: mm/day ## _FillValue: -9999 ## missing_value: -9999 ## time_step: day ## geostatial_lat_min: -50 ## geostatial_lat_max: 50 ## geostatial_lon_min: -180 ## geostatial_lon_max: 180 ## ## 4 dimensions: ## longitude Size:592 ## standard_name: longitude ## long_name: longitude ## units: degrees_east ## axis: X ## latitude Size:679 ## standard_name: latitude ## long_name: latitude ## units: degrees_north ## axis: Y ## time Size:1 *** is unlimited *** ## standard_name: time ## bounds: time_bnds ## units: days since 1980-1-1 0:0:0 ## calendar: standard ## axis: T ## bnds Size:2 (no dimvar) ## ## 18 global attributes: ## CDI: Climate Data Interface version 1.9.0 (http://mpimet.mpg.de/cdi) ## history: Wed Jun 23 10:40:15 2021: cdo -L sellonlatbox,21.81,51.41,-11.72,22.23 -mulc,92 -timmean -selmon,3/5 chirps-v2.0.20210101-20210531.mon_p05_GHA.nc r_MAM_Tot_2021.nc ## Tue Jun 22 15:01:48 2021: cdo monmean chirps-v2.0.20210101-20210531.days_p05_GHA.nc chirps-v2.0.20210101-20210531.mon_p05_GHA.nc ## Tue Jun 22 14:49:19 2021: cdo remapbil,chrips_gha.txt /home/hussen/Downloads/CHIRPS/chirps-v2.0.2021.days_p05.nc chirps-v2.0.20210101-20210531.days_p05_GHA.nc ## created by Climate Hazards Group ## institution: Climate Hazards Group. University of California at Santa Barbara ## Conventions: CF-1.6 ## title: CHIRPS Version 2.0 ## version: Version 2.0 ## date_created: 2021-06-16 ## creator_name: Pete Peterson ## creator_email: pete@geog.ucsb.edu ## documentation: http://pubs.usgs.gov/ds/832/ ## reference: Funk, C.C., Peterson, P.J., Landsfeld, M.F., Pedreros, D.H., Verdin, J.P., Rowland, J.D., Romero, B.E., Husak, G.J., Michaelsen, J.C., and Verdin, A.P., 2014, A quasi-global precipitation time series for drought monitoring: U.S. Geological Survey Data Series 832, 4 p., http://dx.doi.org/110.3133/ds832. ## comments: time variable denotes the first day of the given day. Improved October 2015. ## acknowledgements: The Climate Hazards Group InfraRed Precipitation with Stations development process was carried out through U.S. Geological Survey (USGS) cooperative agreement #G09AC000001 &quot;Monitoring and Forecasting Climate, Water and Land Use for Food Production in the Developing World&quot; with funding from: U.S. Agency for International Development Office of Food for Peace, award #AID-FFP-P-10-00002 for &quot;Famine Early Warning Systems Network Support,&quot; the National Aeronautics and Space Administration Applied Sciences Program, Decisions award #NN10AN26I for &quot;A Land Data Assimilation System for Famine Early Warning,&quot; SERVIR award #NNH12AU22I for &quot;A Long Time-Series Indicator of Agricultural Drought for the Greater Horn of Africa,&quot; The National Oceanic and Atmospheric Administration award NA11OAR4310151 for &quot;A Global Standardized Precipitation Index supporting the US Drought Portal and the Famine Early Warning System Network,&quot; and the USGS Land Change Science Program. ## ftp_url: ftp://chg-ftpout.geog.ucsb.edu/pub/org/chg/products/CHIRPS-latest/ ## website: http://chg.geog.ucsb.edu/data/chirps/index.html ## faq: http://chg-wiki.geog.ucsb.edu/wiki/CHIRPS_FAQ ## frequency: mon ## CDO: Climate Data Operators version 1.9.0 (http://mpimet.mpg.de/cdo) # the 2021 observation is named differently... setnames(dt_obs2021,c(&#39;longitude&#39;,&#39;latitude&#39;,&#39;precip&#39;),c(&#39;lon&#39;,&#39;lat&#39;,&#39;prec&#39;)) # ... and it is on higher resolution: ggplot_dt(dt_obs2021,&#39;prec&#39;,high = &#39;blue&#39;,midpoint = 0) ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. # we can upscale it to half-degree-resolution using the following function: dt_obs2021 = upscale_to_half_degrees(dt_obs2021,uscol = &#39;prec&#39;,bycols = &#39;time&#39;) ## [1] &quot;1/1&quot; ggplot_dt(dt_obs2021,high = &#39;blue&#39;,midpoint = 0) # the time format is different for the two observation data tables, see netcdf description above. # For dt_obs the format is month since date, and can be changed to year-month like this: dt_obs = MSD_to_YM(dt_obs) # For dt_obs2021 it&#39;s the number of days since 1980-01-01, and we can do the following: dt_obs2021[,date := as.Date(time,origin = &#39;1980-01-01&#39;)] # see netcdf description above dt_obs2021[,year := year(date)][,month := month(date)] print(dt_obs2021) ## lon lat prec time date year month ## 1: 21.5 -12.0 NA 15080.5 2021-04-15 2021 4 ## 2: 21.5 -11.5 NA 15080.5 2021-04-15 2021 4 ## 3: 21.5 -11.0 NA 15080.5 2021-04-15 2021 4 ## 4: 21.5 -10.5 NA 15080.5 2021-04-15 2021 4 ## 5: 21.5 -10.0 NA 15080.5 2021-04-15 2021 4 ## --- ## 4266: 51.5 20.5 25.20947 15080.5 2021-04-15 2021 4 ## 4267: 51.5 21.0 21.71183 15080.5 2021-04-15 2021 4 ## 4268: 51.5 21.5 23.18140 15080.5 2021-04-15 2021 4 ## 4269: 51.5 22.0 23.29202 15080.5 2021-04-15 2021 4 ## 4270: 51.5 22.5 NA 15080.5 2021-04-15 2021 4 # Next, let&#39;s restrict 2021-observations to locations that are not blanked out in the past observations: na_locs = dt_obs[year == year[1],.(lon,lat,is.na(prec))] print(na_locs) ## lon lat V3 ## 1: 20.5 -13.5 TRUE ## 2: 21.0 -13.5 TRUE ## 3: 21.5 -13.5 TRUE ## 4: 22.0 -13.5 TRUE ## 5: 22.5 -13.5 TRUE ## --- ## 5078: 51.0 24.5 TRUE ## 5079: 51.5 24.5 TRUE ## 5080: 52.0 24.5 TRUE ## 5081: 52.5 24.5 TRUE ## 5082: 53.0 24.5 TRUE dt_obs2021 = merge(dt_obs2021,na_locs,by = c(&#39;lon&#39;,&#39;lat&#39;)) dt_obs2021 = dt_obs2021[!(V3)] # only keep rows for which V3 is FALSE ggplot_dt(dt_obs2021, &#39;prec&#39;,high = &#39;blue&#39;,midpoint = 0) #delete what we don&#39;t need and bind together: dt_obs[,month:=NULL] dt_obs2021[,c(&#39;month&#39;,&#39;time&#39;,&#39;date&#39;,&#39;V3&#39;):=NULL] dt_obs = rbindlist(list(dt_obs,dt_obs2021),use.names = TRUE) dt_obs = dt_obs[!is.na(prec)] # in which climatology tercile lies the observation for which year? dt_obs = add_tercile_cat(dt_obs) # let&#39;s also add the climatology for alter use: dt_obs[,clim := mean(prec),by = .(lon,lat)] ggplot_dt(dt_obs[year == 2021],&#39;tercile_cat&#39;,low = &#39;red&#39;,high = &#39;blue&#39;) # merge prediction and corresponding observation: dt = merge(dt,dt_obs[year == 2021],by = c(&#39;lon&#39;,&#39;lat&#39;)) # transform percentage prediction to probabilities between zero and one: dt[,normal := normal/100] dt[,above := above/100] dt[,below := below/100] print(dt) ## lon lat normal above below prec year tercile_cat ## 1: 22.0 -11.5 0.2794044 0.3959641 0.3246315 271.66216 2021 1 ## 2: 22.0 -11.0 0.3176142 0.3509704 0.3314154 279.13827 2021 1 ## 3: 22.0 -10.5 0.2897301 0.3781255 0.3321443 300.32019 2021 1 ## 4: 22.0 -10.0 0.3133837 0.3520903 0.3345260 332.45370 2021 1 ## 5: 22.0 -9.5 0.3076811 0.3480890 0.3442299 407.19163 2021 1 ## --- ## 2939: 51.5 20.0 NA NA NA 26.43442 2021 1 ## 2940: 51.5 20.5 NA NA NA 25.20947 2021 1 ## 2941: 51.5 21.0 NA NA NA 21.71183 2021 1 ## 2942: 51.5 21.5 NA NA NA 23.18140 2021 1 ## 2943: 51.5 22.0 NA NA NA 23.29202 2021 1 ## clim ## 1: 200.78224 ## 2: 218.82563 ## 3: 219.82354 ## 4: 224.44427 ## 5: 237.77492 ## --- ## 2939: 14.02317 ## 2940: 13.39437 ## 2941: 13.75870 ## 2942: 14.37246 ## 2943: 17.47911 How to evaluate this dataset will be discussed in Section 5.2.1. Besides the function upscale_to_half_degrees there is another function called upscale_nested_griddings that is more general purpose and makes weaker assumptions about the grids, 4.1.4 Example: preparing data for evaluating exceedence probabilities Here we show how to prepare data for evaluating exceedence probabilities, see Section 5.3. fn = &#39;PrecRegPeXcd_3monthSeasonal.nc&#39; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/PrecRegPeXcd_3monthSeasonal.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float pexcd[lon,lat,model,lead,rthr] ## thrhold4: 400 ## thrhold3: 350 ## thrhold2: 300 ## thrhold1: 200 ## units: % ## _FillValue: -9999 ## ## 6 dimensions: ## time Size:0 *** is unlimited *** (no dimvar) ## rthr Size:4 ## lead Size:6 ## units: months since 2021-2-1 0:0 ## model Size:9 ## names: GEM-NEMO CanCM4i NASA-GEOSS2S GFDL-SPEAR COLA-RSMAS-CCSM4 NCEP-CFSv2 ECMWF Meteo_France UKMO ## lat Size:77 ## units: degrees_north ## lon Size:66 ## units: degrees_east ## ## 5 global attributes: ## modelnames: GEM-NEMO CanCM4i NASA-GEOSS2S GFDL-SPEAR COLA-RSMAS-CCSM4 NCEP-CFSv2 ECMWF Meteo_France UKMO ## nmodels: 9 ## initial_time: Feb2021 ## creation_date: Thu Feb 25 19:58:57 EAT 2021 ## title: Forecast probabilities of Exceedance print(dt) ## lon lat model lead rthr pexcd ## 1: 20.5 -13.5 0 0 0 NA ## 2: 21.0 -13.5 0 0 0 NA ## 3: 21.5 -13.5 0 0 0 NA ## 4: 22.0 -13.5 0 0 0 NA ## 5: 22.5 -13.5 0 0 0 NA ## --- ## 1097708: 51.0 24.5 8 5 3 NA ## 1097709: 51.5 24.5 8 5 3 NA ## 1097710: 52.0 24.5 8 5 3 NA ## 1097711: 52.5 24.5 8 5 3 NA ## 1097712: 53.0 24.5 8 5 3 NA # first, note that the &#39;model&#39;, &#39;rthr&#39;, and &#39;month&#39; column do not make much sense before we insert # the information we gather from the netcdf description.: modelnames = c(&#39;GEM-NEMO&#39;, &#39;CanCM4i&#39;, &#39;NASA-GEOSS2S&#39;, &#39;GFDL-SPEAR&#39;, &#39;COLA-RSMAS-CCSM4&#39;, &#39;NCEP-CFSv2&#39;, &#39;ECMWF&#39;, &#39;Meteo_France&#39;, &#39;UKMO&#39;) thresholds = c(200,300,350,400) dt[,model := modelnames[model + 1]] dt[,rthr := thresholds[rthr + 1]] dt[,month :=lead + 2][,lead:=NULL] Ultimately, we want to compare the skill of these models to a climatological forecast. The climatological forecast for the exceedence probability is just the fraction of observed years where the threshold was exceeded. To calculate this, we require past observations. These are not contained in our example data folder, so we cheat a little here and load in some chirps data. You can get this data (on high resolution) from here: http://digilib.icpac.net/SOURCES/.ICPAC/.CHIRPS-BLENDED/.monthly/.rainfall/.precipitation/. It can then be upscaled as shown in the last section. Our observation data looks like this: print(dt_chirps) ## lon lat prec month year ## 1: 21.5 -12.0 NA 2 1981 ## 2: 21.5 -11.5 NA 2 1981 ## 3: 21.5 -11.0 NA 2 1981 ## 4: 21.5 -10.5 NA 2 1981 ## 5: 21.5 -10.0 NA 2 1981 ## --- ## 720284: 51.5 21.5 4.000000 5 2021 ## 720285: 51.5 22.0 3.356522 5 2021 ## 720286: 51.5 22.5 1.117393 5 2021 ## 720287: 51.5 23.0 1.510869 5 2021 ## 720288: 51.5 23.5 NA 5 2021 In order to get the climatological exceedence probabilities, we can use the following function: clim_fc = climatology_threshold_exceedence(dt_chirps, obs_col = &#39;prec&#39;, thresholds = unique(dt[,rthr]), by_cols = c(&#39;month&#39;,&#39;lon&#39;,&#39;lat&#39;)) print(clim_fc) ## month lon lat year pexcd threshold ## 1: 2 21.5 -12.0 1981 NA 200 ## 2: 2 21.5 -11.5 1981 NA 200 ## 3: 2 21.5 -11.0 1981 NA 200 ## 4: 2 21.5 -10.5 1981 NA 200 ## 5: 2 21.5 -10.0 1981 NA 200 ## --- ## 2881148: 5 51.5 21.5 2021 0 400 ## 2881149: 5 51.5 22.0 2021 0 400 ## 2881150: 5 51.5 22.5 2021 0 400 ## 2881151: 5 51.5 23.0 2021 0 400 ## 2881152: 5 51.5 23.5 2021 NA 400 Note that we passed the thresholds given in dt. The bycols argument tells the function what columns to group by when computing the climatology. Finally, we need to merge the predictions, the climatological forecast and the observation into one data table. Since we only have predictions for 2021, it is enough to provide the climatology forecast and observation for 2021 as well. Also note that we have predictions for more months than observations (at the time this is written), so we cut the predictions for June and July out - we cannot evaluate predictions we don’t know the outcome for. setnames(clim_fc,c(&#39;pexcd&#39;,&#39;threshold&#39;),c(&#39;clim&#39;,&#39;rthr&#39;)) dt = merge(dt,clim_fc[year == 2021,],by = c(&#39;lon&#39;,&#39;lat&#39;,&#39;month&#39;,&#39;rthr&#39;)) dt = merge(dt,dt_chirps[year == 2021],by = c(&#39;lon&#39;,&#39;lat&#39;,&#39;month&#39;,&#39;year&#39;)) #finally, for evaluation we generally work with probabilities between 0 and 1, not percentages: range(dt[,pexcd],na.rm = TRUE) # confirm that the data table contains percentages at the moment... ## [1] 0 100 dt[,pexcd := pexcd/100] #... and correct print(dt) ## lon lat month year rthr model pexcd clim prec ## 1: 21.5 -12.0 2 2021 200 GEM-NEMO 0.996 NA NA ## 2: 21.5 -12.0 2 2021 200 CanCM4i 0.995 NA NA ## 3: 21.5 -12.0 2 2021 200 NASA-GEOSS2S 0.996 NA NA ## 4: 21.5 -12.0 2 2021 200 GFDL-SPEAR 0.990 NA NA ## 5: 21.5 -12.0 2 2021 200 COLA-RSMAS-CCSM4 0.993 NA NA ## --- ## 632444: 51.5 23.5 5 2021 400 COLA-RSMAS-CCSM4 0.000 NA NA ## 632445: 51.5 23.5 5 2021 400 NCEP-CFSv2 0.000 NA NA ## 632446: 51.5 23.5 5 2021 400 ECMWF 0.000 NA NA ## 632447: 51.5 23.5 5 2021 400 Meteo_France 0.000 NA NA ## 632448: 51.5 23.5 5 2021 400 UKMO 0.000 NA NA How to evaluate the predictions from here is discussed in Section 5.3. "],["validation.html", "5 Validation 5.1 Evaluating cross-validation predictions 5.2 Evaluating Tercile Forecasts 5.3 Exceedence probabilities 5.4 Temperature", " 5 Validation In this section we look into validating different types of predictions. Our focus herby lies on proper scoring rules, as well as skill scores for comparison to climatology. 5.1 Evaluating cross-validation predictions Here we evaluate the cross-validation data prepared in Section 4.1.1. The data table contains observations for past years for the seasons MAM and FMA, along with ‘best-guess-predictions’, meaning that they are single numbers, not probabilities: print(dt_cv) ## lon lat season prediction observation year month ## 1: 20.5 -11.5 FMA 316.19452 369.36932 1982 2 ## 2: 20.5 -11.5 MAM 202.94411 208.28058 1982 2 ## 3: 20.5 -11.5 FMA 316.20178 252.47144 1983 2 ## 4: 20.5 -11.5 MAM 205.24921 161.22548 1983 2 ## 5: 20.5 -11.5 FMA 317.43375 267.44031 1984 2 ## --- ## 167330: 51.5 22.5 FMA 25.44651 19.71902 2012 2 ## 167331: 51.5 22.5 FMA 25.59836 27.55773 2013 2 ## 167332: 51.5 22.5 FMA 26.03941 25.14965 2014 2 ## 167333: 51.5 22.5 FMA 26.03053 22.23634 2015 2 ## 167334: 51.5 22.5 FMA 26.00327 34.84376 2016 2 Such predictions are often called point forecasts, whereas forecasts specifying probabilities are called probabilistic. We already have the data in the shape we want it to be, containing both predictions and observations as one column each. Let’s have a look at the bias in our predictions: ### check out local biases: bias_dt = dt_cv[,.(bias = mean(prediction - observation)), by = .(lon,lat,season)] # grouping by lon,lat, and season means that the mean is taken over all years. bias_dt[,range(bias)] # get an idea of the range for plotting ## [1] -12.64276 15.80456 rr = c(-15,15) # fix range, to make plots comparable pp1 = ggplot_dt(bias_dt[season == &#39;FMA&#39;], data_col = &#39;bias&#39;, rr = rr, # fix range to make it comparable to pp2 mn = &#39;bias of FMA prediction&#39;, midpoint = 0) pp2 = ggplot_dt(bias_dt[season == &#39;MAM&#39;], data_col = &#39;bias&#39;, rr = rr, mn = &#39;bias of MAM prediction&#39;, midpoint = 0) # show plots: ggpubr::ggarrange(pp1,pp2) We can use the function MSESS_dt to compute MSE skill scores. The skill is computed relative to leave-one-year-out climatology, which is calculated automatically in the process. ### analyze mean square error skill scores msess = MSESS_dt(dt_cv, fc_col = &#39;prediction&#39;, # column name of forecasts obs_col = &#39;observation&#39;, # column name of observations by_cols = c(&#39;lon&#39;,&#39;lat&#39;,&#39;season&#39;)) # the skill scores should be computed for each location and each season separately # get range for plotting: msess[,range(MSESS)] ## [1] -0.3447786 0.3436327 rr = c(-0.35,0.35) pp1 = ggplot_dt(msess[season == &#39;FMA&#39;], data_col = &#39;MSESS&#39;, rr=rr, mn = &#39;MSE skill score, FMA&#39;) pp2 = ggplot_dt(msess[season == &#39;MAM&#39;], data_col = &#39;MSESS&#39;, rr=rr, mn = &#39;MSE skill score, MAM&#39;) ggarrange(pp1,pp2) Here, as for all skill scores, positive values indicate that the prediction has higher skill than climatology, negative values indicates lower skill. Skill scores are moreover ‘standardized’ such that a score of 1 corresponds to a perfect forecast. Note that there is also a (faster) function MSE_dt if we’re not interested in skill scores, but simply want to compute MSEs. Both function can also handle ensemble predictions, see function documentation. If we want to analyze results by countries, we can use the function add_country_names that adds a column with country names to the data table: # check out average MSEs and MSESSs per country: msess = add_country_names(msess) print(msess) ## lon lat season MSE clim_MSE MSESS country ## 1: 23.0 11.0 MAM 303.14287 308.54726 0.017515586 Sudan ## 2: 23.5 9.0 MAM 751.99749 731.22747 -0.028404319 Sudan ## 3: 23.5 10.5 MAM 294.52421 297.75507 0.010850729 Sudan ## 4: 23.5 11.0 MAM 229.63428 228.55108 -0.004739428 Sudan ## 5: 24.0 9.0 MAM 426.41982 400.60929 -0.064428203 Sudan ## --- ## 2703: 50.0 11.0 MAM 185.52785 200.07593 0.072712761 Somalia ## 2704: 50.5 9.5 MAM 48.67772 46.13510 -0.055112437 Somalia ## 2705: 50.5 10.0 MAM 28.67013 27.39041 -0.046721342 Somalia ## 2706: 50.5 11.0 MAM 55.81477 54.05239 -0.032604992 Somalia ## 2707: 50.5 11.5 MAM 60.25333 60.52558 0.004498041 Somalia msess_by_country = msess[,.(MSE = mean(MSE), MSESS = mean(MSESS)), by = country] # take averages by country print(msess_by_country) ## country MSE MSESS ## 1: Sudan 358.0374 0.015229343 ## 2: South Sudan 901.2060 0.021752470 ## 3: Rwanda 1657.1758 0.129892834 ## 4: Tanzania 3588.8147 0.037472556 ## 5: Burundi 2263.1621 0.110301016 ## 6: Uganda 1578.1713 0.044870020 ## 7: Ethiopia 1863.0355 0.049708565 ## 8: Kenya 2404.1271 0.061263744 ## 9: Eritrea 447.6274 0.009834729 ## 10: Somalia 1121.8166 0.023641155 ## 11: Djibouti 111.1771 0.029694437 Skill scores strongly depend on the skill of the climatological prediction, see Section 5.3. This makes it somewhat problematic to average them in space, as skill scores for different grid points with different climatologies have different meanings. A more appropriate way to see whether the prediction outperformed climatology on average for a given country is by considering average score differences: # positive values indicate better performance than climatology: msess[,.(score_diff = mean(clim_MSE - MSE)),by = country] ## country score_diff ## 1: Sudan 3.9749273 ## 2: South Sudan 26.8968614 ## 3: Rwanda 267.0480866 ## 4: Tanzania 100.4944114 ## 5: Burundi 277.8132419 ## 6: Uganda 69.6647566 ## 7: Ethiopia 91.0034502 ## 8: Kenya 172.7090383 ## 9: Eritrea -0.1503512 ## 10: Somalia 28.1771594 ## 11: Djibouti 5.0474522 The MSE (and its associated skill score) penalizes both systematic forecast errors (i.e. biases) and non-systematic forecast errors. The latter are a consequence of general forecast uncertainty and there is no easy way to reduce them. Biases, however, can often be removed through statistical post-processing, and it is therefore interesting to consider measures for forecast performance that penalize only non-systematic forecast errors, thus giving an idea of the potential skill of a forecast system. The standard metric to assess the potential skill is the Pearson correlation coefficient (PCC). This is the usual correlation coefficient where forecasts and observations are standardized by their respective climatological means and standard deviations, and then the average product of these standardized variables is calculated. The function PCC_dt performs these calculations and is used in the same way as MSESS_dt above. ### calculate Pearson correlation coefficients PCC = PCC_dt(dt_cv, fc_col = &#39;prediction&#39;, # column name of forecasts obs_col = &#39;observation&#39;, # column name of observations by_cols = c(&#39;lon&#39;,&#39;lat&#39;,&#39;season&#39;)) # the correlation coefficient should be computed for each location and each season separately # the maximal range for a correlation coefficient is [-1,1], but sometimes it is useful to narrow it: rr = c(-0.75,0.75) pp1 = ggplot_dt(PCC[season == &#39;FMA&#39;], data_col = &#39;rho&#39;, rr=rr, mn = &#39;Pearson correlation coefficient, FMA&#39;) pp2 = ggplot_dt(PCC[season == &#39;MAM&#39;], data_col = &#39;rho&#39;, rr=rr, mn = &#39;Pearson correlation coefficient, MAM&#39;) ggarrange(pp1,pp2) While there is no technical requirement that the forecasts and observations follow a particular probability distribution when the Pearson correlation coefficient is employed, this metric is best suited for continuous distributions (i.e. it is unlikely to encounter duplicate values) that are relatively symmetric around the mean. For shorter (e.g. weekly) accumulation periods and in dry climates, the distribution of precipitation usually becomes rather skewed and contains a number of zeros. A new metric, the coefficient of predictive ability (CPA), has recently been developed and constitutes an excellent alternative to the PCC as a measure of potential forecast skill in that situation of strongly asymmetric distributions with multiple identical values. See here for more background information about the CPA. The function CPA_dt performs the calculations and is used in the same way as MSESS_dt and PCC_dt above. ### calculate coefficient of predictive ability CPA = CPA_dt(dt_cv, fc_col = &#39;prediction&#39;, # column name of forecasts obs_col = &#39;observation&#39;, # column name of observations by_cols = c(&#39;lon&#39;,&#39;lat&#39;,&#39;season&#39;)) # the CPA should be computed for each location and each season separately # the maximal range for the CPA is [0,1] # a value of 0.5 corresponds to no skill (more details can be found in the document under the link given above) rr = c(0,1) pp1 = ggplot_dt(CPA[season == &#39;FMA&#39;], data_col = &#39;cpa&#39;, rr=rr, mn = &#39;Coefficient of predictive ability, FMA&#39;) pp2 = ggplot_dt(CPA[season == &#39;MAM&#39;], data_col = &#39;cpa&#39;, rr=rr, mn = &#39;Coefficient of predictive ability, MAM&#39;) ggarrange(pp1,pp2) Just like the MSESS, PCC and CPA can be averaged by country using the function add_country_names: # check out average PCCs and CPAs per country: PCC = add_country_names(PCC) CPA = add_country_names(CPA) PCC_by_country = PCC[,.(rho = mean(rho)), by = country] CPA_by_country = CPA[,.(cpa = mean(cpa)), by = country] print(PCC_by_country) ## country rho ## 1: Sudan -0.19636469 ## 2: South Sudan -0.16295459 ## 3: Rwanda 0.30049770 ## 4: Tanzania -0.05635683 ## 5: Burundi 0.23708677 ## 6: Uganda 0.01509619 ## 7: Ethiopia 0.01459451 ## 8: Kenya 0.07685737 ## 9: Eritrea -0.14705925 ## 10: Somalia -0.08751838 ## 11: Djibouti -0.05427490 print(CPA_by_country) ## country cpa ## 1: Sudan 0.3989506 ## 2: South Sudan 0.4130657 ## 3: Rwanda 0.6490021 ## 4: Tanzania 0.4802252 ## 5: Burundi 0.6187519 ## 6: Uganda 0.5011563 ## 7: Ethiopia 0.5158528 ## 8: Kenya 0.5364750 ## 9: Eritrea 0.4310610 ## 10: Somalia 0.4713838 ## 11: Djibouti 0.5031902 5.2 Evaluating Tercile Forecasts Next, we’ll turn our attention to one of the main products disseminated at GHACOFs, the probabilistic forecasts whether the coming season will see a below normal-, normal-, or above normal amount of rainfall. Since these three categories are defined by climatological terciles, we call them tercile forecasts. From an evaluation perspective, there are two different scenarios: Either we get the prediction as a vector of three probabilities, or we just get the probability for the most likely category. Evaluating a vector of three probabilities is preferrable, because it conveys more detailed information about the forecast: Say, for example, two competing models predicted the probabilities (0.5, 0.3, 0.2) and (0.5, 0.49, 0.01), respectively (in the order below, normal, high). Say now, after observing the predicted season, it turns out that the rainfall was in fact above normal. In this case, both predictions were pretty bad, but the first model at least assigned a 20% chance to above-normal-rainfall, whereas the second model only assigned a 1% chance to that outcome. So the first prediction was substantially better. However, if we only look at the category with the highest predicted probability, the two models can’t be distinguished, as they both appear as (0.5,-,-). Therefore, considering all three probabilities of the prediction allows for better forecast evaluation. This does not mean, however, that the communication of the prediction to the public needs to contain all three probabilities, which would likely be more confusing than helpful. In the next subsection we’ll discuss how to evaluate a fully probability forecast (vector of three probabilities). In the section thereafter, we address the case where only the most likely category is known. 5.2.1 Proper scoring rules for full tercile forecasts Proper scoring rules are tools for evaluating predictive performance. Given a prediction and the corresponding observation, a proper score returns a single number. We consider negatively oriented scores, that is, lower scores indicate better performance. Popular examples are the Brier Score, Mean Square Error (MSE), Log-likelihood score or the continuous ranked probability score (CRPS). When we’re dealing with tercile forecasts of precipitation, we can use the Multicategory Brier Score (MBS). It is defined as \\[\\text{MBS} := (p_1 - e_1)^2 + (p_2 - e_2)^2 + (p_3 - e_3)^2.\\] Here, \\(p_1,p_2,\\) and \\(p_3\\) are the predicted probabilities for the three categories, and \\(e_i\\) is 1 if the observation falls in the \\(i\\)th category, and 0 else. For example, if the observation falls into the first category, the MBS would be \\[(p_1 - 1)^2 + p_2^2 + p_3^2.\\] This score is strictly proper, meaning that it rewards calibration and accuracy. In our particular situation, the climatological forecast is uniform (since climatology is used to define the tercile categories), and the climatological forecast (1/3,1/3,1/3) always gets a MBS of 2/3. It is therefore very convenient to consider the Multicategory Brier Skill Score (MBSS) \\[MBSS := \\frac{3}{2}(2/3 - \\text{MBS}).\\] Like other skill scores, this score is normalized in the sense that a perfect forecaster attains a skill score of 1 and a climatology forecast always gets a skill score of 0. Note that, for the MBSS, higher values indicate better performance, unlike for the MBS (similar as for other scores such as MSE). Tercile forecasts are a particular situation where the skill score is a strictly proper scoring rule itself (albeit positively oriented). This means in particular that we may average Multicategory Brier Skill Scores accross different grid points without being concerned about different scales of precipitation. If, for example, the average MBSS of our prediction over all gridpoints in Ethiopia is above 0, our prediction for Ethiopia was on average better than climatology. Let’s now look at a data example, contained in the data_dir specified here. The core function is simply called MBSS_dt. The main work is organizing the data in one data table of the correct format, which was done in Section 4.1.3. In particular, recall that we can use the function add_tercile_cat to determine which observations are in the lower or upper climatology tercile. dt = dt_tercile_forecast print(dt) ## lon lat normal above below prec year tercile_cat ## 1: 22.0 -11.5 0.2794044 0.3959641 0.3246315 271.66216 2021 1 ## 2: 22.0 -11.0 0.3176142 0.3509704 0.3314154 279.13827 2021 1 ## 3: 22.0 -10.5 0.2897301 0.3781255 0.3321443 300.32019 2021 1 ## 4: 22.0 -10.0 0.3133837 0.3520903 0.3345260 332.45370 2021 1 ## 5: 22.0 -9.5 0.3076811 0.3480890 0.3442299 407.19163 2021 1 ## --- ## 2939: 51.5 20.0 NA NA NA 26.43442 2021 1 ## 2940: 51.5 20.5 NA NA NA 25.20947 2021 1 ## 2941: 51.5 21.0 NA NA NA 21.71183 2021 1 ## 2942: 51.5 21.5 NA NA NA 23.18140 2021 1 ## 2943: 51.5 22.0 NA NA NA 23.29202 2021 1 ## clim ## 1: 200.78224 ## 2: 218.82563 ## 3: 219.82354 ## 4: 224.44427 ## 5: 237.77492 ## --- ## 2939: 14.02317 ## 2940: 13.39437 ## 2941: 13.75870 ## 2942: 14.37246 ## 2943: 17.47911 # get Multicategory Brier Skill Score: mbss = MBSS_dt(dt,obs_col = &#39;tercile_cat&#39;) ggplot_dt(mbss,high = &#39;darkgreen&#39;,low = &#39;purple&#39;,discrete_cs = TRUE,binwidth = 0.2,midpoint = 0, mn = &#39;MBSS for MAM tercile forecast 2021&#39;) Areas colored in green show where the prediction was better than climatology, areas colored in purple indicate worse performance. The MBSS indicates, for example, good forecast performance over most of Tanzania. To see whether the forecast was overall better than climatology, we average the MBSS: # check out the MBSS by country: mbss = add_country_names(mbss) mean_mbss = mbss[,.(mean_mbss = mean(MBSS,na.rm = T)), by = country] print(mean_mbss) ## country mean_mbss ## 1: Sudan 0.098439671 ## 2: South Sudan 0.092564026 ## 3: Rwanda 0.061607937 ## 4: Tanzania 0.166882194 ## 5: Burundi 0.114454899 ## 6: Uganda 0.118914478 ## 7: Ethiopia 0.004924431 ## 8: Kenya 0.071575657 ## 9: Eritrea -0.030340763 ## 10: Somalia 0.037938177 ## 11: Djibouti -0.015321119 Finally, let’s check whether this makes sense, by comparing climatology to the prediction: dt[,anomaly:= prec - clim] ggplot_dt(dt[year == 2021],&#39;anomaly&#39;,high = &#39;blue&#39;,low = &#39;red&#39;,midpoint = 0, mn = &#39;observed 2021 MAM precip anomaly&#39;) # or, as discrete plot: pp1 = ggplot_dt(dt[year == 2021],&#39;anomaly&#39;, high = &#39;blue&#39;,low = &#39;red&#39;,midpoint = 0, rr = c(-100,100),discrete_cs = TRUE,breaks = seq(-100,100,40), mn = &#39;observed 2021 MAM precip anomaly&#39;) # also, let&#39;s plot the predicted probabilities: pp2 = ggplot_dt(dt,&#39;below&#39;,midpoint = 0.33,discrete_cs = TRUE,binwidth = 0.05,mn = &#39;predicted probability below&#39;) pp3 = ggplot_dt(dt,&#39;normal&#39;,midpoint = 0.33,discrete_cs = TRUE,binwidth = 0.05,mn = &#39;predicted probability normal&#39;) pp4 = ggplot_dt(dt,&#39;above&#39;,midpoint = 0.33,discrete_cs = TRUE,binwidth = 0.05,mn = &#39;predicted probability above&#39;) ggpubr::ggarrange(pp1,pp2,pp3,pp4,ncol = 4) As we can see, the season was very wet overall. The prediction was overall wet as well, especially over the western part of the considered region, where the prediction also got assigned a positive MBSS. 5.2.2 Evaluation when only the highest probability category is avaliable As argued above, it is preferrable to evaluate tercile forecasts that are given as full probability vector containing all three probabilities. However, we might still face scenarios where we only have the highest probability category available, e.g. some older forecasts for which only this has been saved. What can we do in this case? Intuitively, a promising candidate for a proper score seems to be the two-category-Brier score on the category with the highest probability \\[BS_{\\max} = (p_{\\max}-e_{\\max})^2,\\] where \\(p_{\\max}\\) is the probability assigned to the maximum probability category, and \\(e_{\\max} = 1\\) if the observation falls into that category and \\(0\\) else. Unfortunately, it turns out that this score is improper: it does not reward calibration and accuracy. Let us look at an example forecast for just one gridpoint: In this example, we compare a near-climatological forecast (red) with a prediction issued by a forecaster (blue). The highest probability categories are indicated by the shaded area: for the forecaster it is the ‘above normal’ category, for the climatology-forecast the ‘below normal’ category. Below the figure, the scores achieved by the forecaster and climatology are shown for all three possible outcomes. The climatology gets a better (lower) Brier score when the observation is ‘normal’ or ‘above normal’. This is paradoxical, since the forecaster assigned higher probabilities to these categorie. This highlights the improperness of the max-Brier score: When evaluating predictions with this score, the best forecast does usually not get preferred. This is unintuitive, because the (standard) Brier score is proper. However, the Brier score is designed for predictions of two-category-events with fixed categories. In the definition of \\(BS_{\\max}\\) the categories are ‘highest probability category’ vs. the rest. Therefore, the two categories depend on the forecast probabilities and therefore may vary between different predictions. This makes the Brier score improper. However, a nice application of Theorem 1 of this paper shows that there is a class of proper scoring rules that can be evaluated, when only the probability of the most likely category is known. For example, we can use the score \\[ cBS_\\max:= p^2_{\\max} - 2p_\\max e_\\max + 1.\\] Note that this score satisfies \\(cBS_\\max=BS_{\\max} - e_\\max +1\\), so it’s a corrected version of the max-Brier score which is proper and avoids the problems above. Adding \\(+1\\) in the definition of the score is not necessary but convenient: it ensures that the score is nonnegative and a perfect score is 0. Usually we want to know whether our prediction outperformed climatology. For most scores we can consider skill scores, but unfortunately this does not work here. Climatology assigns to all three categories equal probabilities (1/3), and therefore does not really have a maximum-probability-category. Thus, the definition of \\(e_\\max\\) makes no sense for a climatological forecaster. However, a reasonable viewpoint is that for a climatological forecast the maximum-probability-category can be picked at random, since all categories are getting assigned the same probability. This means that climatology achieves a score of 4/9 with probability 1/3 (when \\(e_\\max = 1\\)), but only achieves a score of 10/9 with probability 2/3. Thus, on average the climatological forecast achieves a score of \\(\\frac 1 3 \\frac 4 9 + \\frac 23 \\frac {10}9 = \\frac{24}{27}\\). A forecast that attains a \\(cBS_\\max\\) of below 24/27 performs on average better than climatology. We therefore define the ‘skill score’ \\[cBSS_\\max := 1 - \\frac{27}{24}cBS_\\max.\\] Note that this is not a skill score in the strict sense, but can be interpretet similarly: values above 0 indicate higher skill than climatology on average, with a \\(cBSS_\\max\\) of 1 corresponding to a perfect forecast. To try this out in action, let us look at the 2021 tercile forecasts # data_dir = &#39;/nr/project/stat/CONFER/Data/validation/example_data/202102/&#39; # as in section 3 fn = &#39;Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc&#39; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc (NC_FORMAT_CLASSIC): ## ## 3 variables (excluding dimension variables): ## float below[lon,lat] ## average_op_ncl: dim_avg_n over dimension(s): model ## units: ## lead: 1 ## _FillValue: -9999 ## float normal[lon,lat] ## _FillValue: -9999 ## lead: 1 ## units: ## average_op_ncl: dim_avg_n over dimension(s): model ## float above[lon,lat] ## _FillValue: -9999 ## lead: 1 ## units: ## average_op_ncl: dim_avg_n over dimension(s): model ## ## 3 dimensions: ## time Size:0 *** is unlimited *** (no dimvar) ## lat Size:381 ## units: degrees_north ## lon Size:326 ## units: degrees_east ## ## 7 global attributes: ## creation_date: Thu Feb 18 17:06:05 EAT 2021 ## Conventions: None ## source_file: Objective Forecast ## description: Obtained by averaging CPT and local regression ## title: Tercile Consolidated Objective Forecast ## history: Mon Feb 22 10:28:53 2021: ncrename -v LAT,lat Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc ## Mon Feb 22 10:28:43 2021: ncrename -v LON,lon Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc ## Mon Feb 22 10:28:26 2021: ncrename -d LON,lon Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc ## Mon Feb 22 10:27:42 2021: ncrename -d LAT,lat Ens_Prec_1monLead_MAM_Prob_EnsRegrCPT-avg.nc ## NCO: netCDF Operators version 4.9.3 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco) dt = dt[!is.na(below) | !is.na(normal) | !is.na (above)] p1 = ggplot_dt(dt,data_col = &#39;below&#39;, midpoint = dt[,min(below,na.rm = TRUE)]) p2 = ggplot_dt(dt,data_col = &#39;normal&#39;, midpoint = dt[,min(normal,na.rm = TRUE)], high = &#39;darkgoldenrod&#39;) # see https://www.r-graph-gallery.com/ggplot2-color.html for an overview of color names. p3 = ggplot_dt(dt,data_col = &#39;above&#39;, midpoint = dt[,min(above,na.rm = TRUE)], high = &#39;darkgreen&#39;) ggarrange(p1,p2,p3,ncol = 3) ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. In order to evaluate these forecast the high resolution CHIRPS-data of the past is missing! fn = &quot;PredictedProbabilityRain_Mar-May_Feb2021_new.nc&quot; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/PredictedProbabilityRain_Mar-May_Feb2021_new.nc (NC_FORMAT_NETCDF4): ## ## 3 variables (excluding dimension variables): ## float normal[lon,lat] (Contiguous storage) ## _FillValue: -1 ## float above[lon,lat] (Contiguous storage) ## _FillValue: -1 ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 2 ## float below[lon,lat] (Contiguous storage) ## _FillValue: -1 ## lead: 1 ## average_op_ncl: dim_avg_n over dimension(s): model ## type: 0 ## ## 2 dimensions: ## lat Size:77 ## _FillValue: NaN ## units: degrees_north ## lon Size:66 ## _FillValue: NaN ## units: degrees_east dt[,normal := normal/100][,above := above/100][,below := below/100] 5.3 Exceedence probabilities Another forecast product issued at GHACOFs are exceedence probabilities of precipitation for certain thresholds, generally related to crops important for the region. A proper scoring rule based on the predicted exceedence probability \\(p_\\text{exc}(c)\\) of a threshold \\(c\\) is the Brier score of exceedence \\[BS_{ex}(c) := (p_\\text{exc}(c) - 1\\{y&gt;c\\})^2,\\] where \\(1\\{y&gt;c\\}\\) equals 1 if the observation \\(y\\) exceeded threshold \\(c\\), and 0 else. Skill scores for comparison with a climatological forecast can be calculated in the usual way. The climatological forecast for the exceedence probability is the fraction of past observations that exceeded the threshold. In Section 4.1.4 we already derived this dataset: print(dt_prexc) ## lon lat month year rthr model pexcd clim prec ## 1: 21.5 -12.0 2 2021 200 GEM-NEMO 0.996 NA NA ## 2: 21.5 -12.0 2 2021 200 CanCM4i 0.995 NA NA ## 3: 21.5 -12.0 2 2021 200 NASA-GEOSS2S 0.996 NA NA ## 4: 21.5 -12.0 2 2021 200 GFDL-SPEAR 0.990 NA NA ## 5: 21.5 -12.0 2 2021 200 COLA-RSMAS-CCSM4 0.993 NA NA ## --- ## 632444: 51.5 23.5 5 2021 400 COLA-RSMAS-CCSM4 0.000 NA NA ## 632445: 51.5 23.5 5 2021 400 NCEP-CFSv2 0.000 NA NA ## 632446: 51.5 23.5 5 2021 400 ECMWF 0.000 NA NA ## 632447: 51.5 23.5 5 2021 400 Meteo_France 0.000 NA NA ## 632448: 51.5 23.5 5 2021 400 UKMO 0.000 NA NA This dataset contains predictions of exceedence (by different models) for several thresholds (rthr), as well as observed rainfall and a climatological prediction for the exceedence probabilities. This is everything we need to compute the \\(BS_{ex}\\)-skill score. To this end, we have the function BSS_ex_dt. If we would not have a climatological exceedence forecast available, we could have still computed the \\(BS_{ex}\\)-score using the function BS_ex_dt. This is still usefull for comparing competing models (see below), but does not tell us where the prediction is better or worse than climatology. bss_dt = BSS_ex_dt(dt_prexc,fc_col = &#39;pexcd&#39;,threshold_col = &#39;rthr&#39;,obs_col = &#39;prec&#39;,by_cols = c(&#39;model&#39;,&#39;month&#39;,&#39;lon&#39;,&#39;lat&#39;)) print(bss_dt[!is.na(BS_ex)]) ## model month lon lat rthr BS_ex clim_BS_ex ## 1: COLA-RSMAS-CCSM4 2 22.0 -12.0 200 9.000183e-06 0.390625 ## 2: COLA-RSMAS-CCSM4 2 22.0 -12.0 300 9.063039e-01 0.000625 ## 3: COLA-RSMAS-CCSM4 2 22.0 -12.0 350 7.499560e-01 0.000000 ## 4: COLA-RSMAS-CCSM4 2 22.0 -12.0 400 5.041000e-01 0.000000 ## 5: COLA-RSMAS-CCSM4 2 22.0 -11.5 200 9.999695e-07 0.360000 ## --- ## 436468: UKMO 5 51.5 22.5 400 0.000000e+00 0.000000 ## 436469: UKMO 5 51.5 23.0 200 0.000000e+00 0.000000 ## 436470: UKMO 5 51.5 23.0 300 0.000000e+00 0.000000 ## 436471: UKMO 5 51.5 23.0 350 0.000000e+00 0.000000 ## 436472: UKMO 5 51.5 23.0 400 0.000000e+00 0.000000 ## BSS_ex ## 1: 0.9999770 ## 2: -1449.0863070 ## 3: -1.0000000 ## 4: -1.0000000 ## 5: 0.9999972 ## --- ## 436468: 0.0000000 ## 436469: 0.0000000 ## 436470: 0.0000000 ## 436471: 0.0000000 ## 436472: 0.0000000 Skill scores are generally not defined when the climatological prediction is perfect and the climatological score is zero. This happens frequently for exceedence probabilities (e.g. lines 3 and 4 in the data table above) at locations where the considered threshold has never been exceeded in the observation. Simply for plotting reasons we put the skill score to -1 in this case if the prediction scores above 0 (since the climatological prediction wwas better in this case), and to 0 if both climatology and prediction assign a probability of 0. Let us look at the skill scores by the different models for the March forecast for exceedence level 200mm: # make a list of skill score plots: theme_set(theme_bw(base_size = 10)) # smaller font plot_list = list() for(mod in unique(bss_dt[,model])) # 1 plot for each model { plot_list = c(plot_list,list(ggplot_dt( bss_dt[model == mod &amp; month == 3 &amp; rthr == 200], &#39;BSS_ex&#39;, mn = mod, high = &#39;red&#39;, midpoint = 0, rr= c(-1,1), guide = guide_colorbar(title = NULL, barwidth = 75, direction = &#39;horizontal&#39;)))) } ggpubr::ggarrange(plotlist = plot_list,ncol = 3,nrow = 3,common.legend = TRUE,legend = &#39;bottom&#39;) Here, red color indicates better performance of the prediction than climatology. Large areas of the map are blue, which indicates better performance of the climatological forecast than of the prediction models. However, these are mostly areas where the observations never exceeded 200mm. Therefore, the climatological forecast issued a 0% chance of rainfall exceeding 200mm, whereas all actual prediction models issued a small positive probability and therefore performed ‘worse’. This not so much highlights a problem of the forecasts than rather a problem of skill scores, which become degenerate whenever the climatological prediction is near perfect. For comparing overall performance, we can average scores spatially. Note that, because of the above-mentioned effect, it is important not to average skill scores. However, since we have a climatology forecast in our data table, we can compute a spatially averaged score for climatology as well. Thus, we can compare whether the prediction models performed on average better or worse than climatology. mean_scores = bss_dt[,.(BS_ex = mean(BS_ex,na.rm = T)),by = .(model,month,rthr)] # get climatology score as well: mean_clim_score = bss_dt[model == model[1],.(BS_ex = mean(clim_BS_ex,na.rm = T)),by = .(month,rthr)] mean_clim_score[,model := &#39;clim&#39;] mean_scores = rbindlist(list(mean_scores,mean_clim_score),use.names = TRUE) print(mean_scores) ## model month rthr BS_ex ## 1: COLA-RSMAS-CCSM4 2 200 0.3206483211 ## 2: COLA-RSMAS-CCSM4 2 300 0.3043674982 ## 3: COLA-RSMAS-CCSM4 2 350 0.2669425955 ## 4: COLA-RSMAS-CCSM4 2 400 0.2114291366 ## 5: COLA-RSMAS-CCSM4 3 200 0.3896413080 ## --- ## 156: clim 4 400 0.0033205367 ## 157: clim 5 200 0.0263377224 ## 158: clim 5 300 0.0047990815 ## 159: clim 5 350 0.0015935347 ## 160: clim 5 400 0.0006833022 Here, every model gets assigned a single mean score for each month and each threshold (the mean score over all gridpoints). Lower values indicate better overall performance. Let us plot the data: pp = ggplot(mean_scores) + geom_line(aes(x = month,y = BS_ex,color = model,linetype = model)) + facet_wrap(~rthr,nrow = 1) print(pp) The plot shows that, averaging over all grid points, a climatological forecast does much better than all the systems, which probably indicates that the systems need to be bias corrected. 5.4 Temperature In our folder of example data we also have a file containing temperature predictions. The file already contains correlations as well. Here we simply visualize these correlations as plots: fn = &#39;TrefEnsRegr_monthly.nc&#39; dt = netcdf_to_dt(paste0(data_dir,fn)) ## File /nr/project/stat/CONFER/Data/validation/example_data/202102/TrefEnsRegr_monthly.nc (NC_FORMAT_CLASSIC): ## ## 6 variables (excluding dimension variables): ## float below[lon,lat,model,lead] ## units: % ## _FillValue: -9999 ## float above[lon,lat,model,lead] ## units: % ## _FillValue: -9999 ## float normal[lon,lat,model,lead] ## units: % ## _FillValue: -9999 ## float corr[lon,lat,model,lead] ## units: cor ## _FillValue: -9999 ## float tref[lon,lat,model,lead] ## units: K ## _FillValue: -9999 ## float anom[lon,lat,model,lead] ## units: K ## _FillValue: -9999 ## ## 4 dimensions: ## lon Size:66 ## units: degreesE ## long_name: lon ## lat Size:77 ## units: degreesN ## long_name: lat ## model Size:5 ## units: number ## long_name: model ## lead Size:3 ## units: month ## long_name: lead # plot correlations of predictions for all five models at all lead_times: # create list of plots: plot_list = list() for(leadtime in 1:3) { for(mod in 1:5) { plot_list = c(plot_list,list(ggplot_dt(dt[model == mod &amp; lead == leadtime], &#39;corr&#39;, rr = c(-1,1), mn = paste0(&#39;model &#39;,mod,&#39;, lead &#39;,leadtime), discrete_cs = TRUE, binwidth = 0.2, guide = guide_colorbar(title = NULL, barwidth = 75, direction = &#39;horizontal&#39;)))) } } #plot as grid: do.call(&#39;ggarrange&#39;, c(plot_list,ncol = 5,nrow = 3,common.legend = TRUE,legend = &#39;bottom&#39;)) "],["validation-1.html", "6 Validation 6.1 Matching predictions and observations 6.2 Evaluating weekly predictions of precipitation", " 6 Validation This section looks into evaluation of weekly predictions. Seaval can be used to compute evaluation metrics such as the MSE for the running forecasts. These results can be saved as netcdfs which allows to integrate the results with the East African Hazard Watch. 6.1 Matching predictions and observations Numerical weather prediction models produce forecasts on a spatial grid with a certain resolution. Similarly, satellite-based observation datasets, such as for example CHIRPS, provide estimates of observed weather on a spatial grid. It frequently happens, that the observation and predictions are on grids with different resolution. If this is the case, you need to first map the values from the different grids onto the same grid, before they can be compared. The package SeaVal allows to do that by the function upscale_lonlat. 6.2 Evaluating weekly predictions of precipitation The function eval_weekly_precip operates directly on the current format the weekly predictions are stored in. It takes the name of a folder where the predictions are stored, as well as the initialization date in the format ‘YYYYMMDD’. At ICPAC the predictions are stored under /SharedData/wrf/weekly/ "]]
